
%%\documentclass[12pt,oneside]{article}

\documentclass{eptcs}
\providecommand{\event}{Journal TBD}
\providecommand{\publicationstatus}{ } %% TBD - fill in any footer
%%\documentclass[12pt,oneside]{llncs}

%% Language %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{hyperref}   %% sets hyperlinks within a pdf
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{parskip}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{caption}

\usepackage{color}

%%\usepackage{subcaption}

%%\usepackage{lmodern} %Type1-font for non-english texts and characters


%% Packages for Graphics & Figures %%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage{graphicx} %%For loading graphic files
%\usepackage{subfig} %%Subfigures inside a figure
%\usepackage{tikz} %%Generate vector graphics from within LaTeX

%\usepackage{mathptmx} %% Times fonts, for math as well


%%\makeindex

%% The adjustment amounts depend on the font
%% -.5 for 12pt, -.875 for 10pt
%% FIXME: should these be set instead of adjusted?
%% Use these with lncls?
%\addtolength{\oddsidemargin}{-.5in}
%\addtolength{\evensidemargin}{-.5in}
%\addtolength{\textwidth}{1.0in}
%\addtolength{\topmargin}{-.5in}
%\addtolength{\textheight}{1.0in}

\def\titlerunning{SMTEVAL-2013}
\def\authorrunning{Cok, Stump, Weber}

\newcommand{\tjark}[1]{\marginpar{\footnotesize{#1}}}
\newcommand{\tjarkx}[1]{\makebox[0pt][c]{\Large\textcolor{red}{${}^*$}}\marginpar{\footnotesize{#1}}}

\begin{document}

\title{The 2013 SMT Evaluation}

%\author{
%David R. Cok \\ GrammaTech, Inc., USA \\ \email{dcok@grammatech.com} \\ 
%Aaron Stump \\ University of Iowa, IA, USA \\ \email{aaron-stump@uiowa.edu} \\ 
%Tjark Weber \\ Uppsala University, Sweden \\ \email{tjark.weber@it.uu.se} }

%% eptcs format
\author{
David R. Cok \institute{GrammaTech, Inc., USA \email{dcok@grammatech.com}}
\and  
Aaron Stump \institute{University of Iowa, IA, USA \email{aaron-stump@uiowa.edu}}
\and  
Tjark Weber \institute{Uppsala University, Sweden \email{tjark.weber@it.uu.se}}
}

%% lncls format
%\author{David R. Cok \inst{1} \and \\ 
%Aaron Stump \inst{2} \and \\ 
%Tjark Weber \inst{3}  \\
%}
%
%\institute{GrammaTech, Inc., USA \email{dcok@grammatech.com} \and
%University of Iowa, IA, USA \email{aaron-stump@uiowa.edu} \and
%Uppsala University, Sweden, \email{tjark.weber@it.uu.se}
%}

\maketitle
\pagestyle{headings}
\pagenumbering{arabic}
\centerline{\today}

%\abstract{
\begin{abstract}
After 8 years of SMT Competitions, the SMT Steering Committee decided,
for 2013, to sponsor an evaluation of the status of SMT benchmarks and
solvers, rather than another competition.  This report summarizes the
results of the evaluation, conducted by the authors.  The key
observations are that (1)~the competition results are quite sensitive
to randomness and (2)~the most significant need for the future is
assessment and improvement of benchmarks in the light of SMT
applications.  The evaluation also measured competitiveness of
solvers, general coverage of solvers, logics, and benchmarks, and
degree of repeatability of measurements and competitions.
\end{abstract}


\section{Introduction}

\subsection{The Competition history and goals}
From 2005 through 2012 (and coming again in 2014), the SMT community
sponsored an annual competition among SMT solvers
(cf.\ Fig.~\ref{Fig:history}).  The purpose of the competition is to
encourage advances in SMT solver implementations acting on benchmark
formulas of theoretical or practical interest.  Public competitions
are a well-known means of stimulating advancement in software tools.
For example, in automated reasoning, the SAT and CASC competitions for
propositional and first-order reasoning tools, respectively, have
spurred significant innovation in their fields
\cite{leberre+03,PSS02}.  Indeed, the SMT competition increased in
size each year: more benchmarks were added, new solver teams
participated, and more logic divisions were defined.

The particular goals of SMT-COMP include the following\tjarkx{Reference?}:
\begin{itemize}[noitemsep]
\item enable research on SMT solvers by benchmarking and comparing performance;
\item promote a standard format for SMT problems (SMT-LIB v2~\cite{BarST-SMT-10});
\item collect additional benchmarks;
\item identify and develop new theories and logics for SMT, encouraging their inclusion in SMT solvers;
\item introduce SMT users and implementors to each other;
\item provide a forum for SMT implementors to promote their SMT solvers and for SMT users to assess the comparative performance of solvers; and
\item encourage the development of industrial-strength solvers for wide-spread use.
\end{itemize}

\begin{figure}
\centering
\includegraphics[width=1.3\textwidth, angle=90]{SMTCOMP-History}
\caption{SMT-COMP history}
\label{Fig:history}
\end{figure}

\tjark{Fig.~\ref{Fig:history} (and some of the other figs.\ that look
  like they come from PowerPoint/ Excel) require more work.}

\subsection{Concerns prompting an evaluation}

In 2013, the SMT Steering Committee decided to take a collective breath and sponsor an evaluation of the current state of the art, without the pressure of a competition. In particular, the implementation teams found that preparing for a competition required considerable engineering work that detracted from other goals. On the one hand, the engineering work is necessary for users to use the tools as off-the-shelf applications. But holding the competition every year was causing some otherwise highly involved teams to withdraw. Thus there was a collective desire to pause the competition cycle for a year.

Second, there was a desire to evaluate the state of the SMT community and the competition. The competition had become focused on details of performance on a particular set of benchmarks for particular logics and\tjarkx{This contradicts our main conclusion~(1).  Suggestion: insert ``there was a feeling that''} its results had become somewhat predictable. Perhaps other goals need to be reemphasized. For example, more benchmarks from applications are needed, not just crafted challenge problems. In addition, an evaluation of progress of the SMT community was desired, not just winning narrowly focused competitions. Perhaps a variety of metrics would be useful.
 
And third, for many years the competition had relied on the SMT-Exec computational infrastructure\tjarkx{Reference for SMT-Exec?}.  A new infrastructure, Star-Exec \cite{webStarExec} had been funded by the NSF and developed at the University of Iowa, but had not yet been tried out in earnest.  In fact, a goal in~2012 had been to use Star-Exec for SMT-COMP~2012; however, Star-Exec was not sufficiently operational and the competition had to revert to SMT-Exec.  It was anticipated that an evaluation in 2013, with less deadline pressure, would enable using Star-Exec on a shake-down cruise prior to a competition in 2014 at the Federated Logic Conference~(FLoC) Olympic Games \cite{webOlympicGames}.

\subsection{Evaluation goals}\tjark{The section lists \emph{what} we evaluated.  To me this is different from our goals, i.e., \emph{why} did we evaluate these things?  We could say more about the latter, or simply change the title of this subsection, e.g., to ``Evaluation areas.''}

The SMT Steering Committee appointed a team of evaluators (the authors
of this report) to examine areas of interest. The evaluators studied
the following topics:
\begin{itemize}[noitemsep]
\item continuity and turnover in SMT solver participation in past competitions (\ref{Solvers})
\item the performance of all historical and current solvers on the full set of benchmarks, measuring
\begin{itemize}[noitemsep]
\item the improvement in performance over time (\ref{Progress})
\item repeatability of performance measurements (\ref{Accuracy})
\item repeatability of competition results (\ref{CompetitionRepeatability})
\item competitiveness of solvers (\ref{Competitiveness})
\end{itemize}
\item the usefulness of various logics
\begin{itemize}[noitemsep]
\item characteristics of existing logics (\ref{Lattice})
\item which logics are implemented by solvers (\ref{Coverage})
\item which logics are particularly relevant to application areas (\ref{Applications})
\end{itemize}
\item the state of existing benchmarks
\begin{itemize}[noitemsep]
\item the range of computational difficulty of the benchmarks (\ref{Benchmarks})
\item the degree to which benchmarks discriminate among solvers (\ref{Benchmarks})
\item which logics have support in benchmarks (\ref{Benchmarks})
\end{itemize}

\end{itemize}


\section{Evaluation tools}

\subsection{SMT-LIB benchmarks}

One goal of the SMT project since its inception has been collecting benchmarks by which to evaluate SMT solvers and to represent challenge problems in the field. The growth in the number of benchmarks available is shown in the graphics of Fig.~\ref{Fig:history}. {\em All} of the non-incremental\tjarkx{Aren't all of the SMT benchmarks currently present in Star-Exec non-incremental?} benchmarks currently present in Star-Exec (95\,491\tjarkx{I believe we had 95492 benchmarks.  (I remember I discovered one benchmark that was in SMT-LIB but missing from Star-Exec, and Aaron or I added this before the evaluation started.)} benchmarks) were used for the evaluation. Note that in any previous year, only some of these benchmarks were available.\tjarkx{This is a bit misleading.  Our set of benchmarks does not include some benchmarks used for SMT-COMP 2012 that were not incorporated into SMT-LIB (yet).  And according to Fig.~\ref{Fig:history}, there were $> 100,000$ benchmarks already in 2011, while we used less than $100,000$.  How to explain this difference?} Furthermore, the competition each year used a random selection of benchmarks (guided by a difficulty distribution). The fact that the set of benchmarks used in competition was different each year muddied any year-to-year comparison of results. By using all of the benchmarks in the evaluation, we intend that any comparative assessments across years or solvers be more accurate. The various kinds of benchmarks and their distribution across logics are discussed below in Section~\ref{Evaluations}.

\subsection{Solvers}

The SMT competitions required that participating solvers be publicly available Linux applications, and that they be available for any future experimenter to use on new experiments. Thus all historical solvers are still available. However, in 2010, the competition adopted the then new benchmark format, SMT-LIB~v2~\cite{BarST-SMT-10}. Thus solvers prior to 2010 do not run on the current benchmark set. The participating solvers are shown in Fig.~\ref{Fig:solvers}.

\begin{figure}
\centering
\includegraphics{Solvers}
\caption{Solvers used in each year of SMT-COMP and SMT-EVAL. Solvers prior to 2010 (with green backgrounds) do not support SMT-LIB~v2 and were not used for SMT-EVAL; boxes with blue backgrounds identify solvers that are new for the evaluation; all those since 2010 (orange or blue backgrounds) were used for the evaluation.}
\label{Fig:solvers}
\end{figure}

\tjark{Fig.~\ref{Fig:solvers}: I don't understand the order of
  solvers.  Also, I don't understand the total number of solvers for
  2005 and 2006.}

SMT-EVAL used all historical solvers since~2010 (32~total), added 9 versions of previous solvers that were updated in~2013, and included 4 additional experimental solvers,\footnote{These were four variations of a portfolio-style solver from Abziz~\cite{aziz:msc,aziz2012machine}.} for a total of~45 solvers.  All solver implementation teams that we could reach were apprised of the upcoming evaluation and given the opportunity to submit new versions of their solvers. Some teams simply submitted the current version of their solver or advised us to download the current public version from the team's website. Thus the solvers are not necessarily tuned to particular application domains or for competition. Any comparisons for particular applications or kinds of benchmark problems should perform an independent analysis.

\subsection{Star-Exec}

SMT-EVAL successfully used the new Star-Exec computational framework \cite{webStarExec,StuST-IJCAR-14}. Running SMT-EVAL on Star-Exec did indeed expose a number of bugs and user interface issues; these were corrected in the course of the SMT-EVAL runs. Thus SMT-EVAL served a valuable purpose in preparing Star-Exec for larger scale use.

TBD:: Can Aaron say more about what was learned?? More information about the characteristics of Star-Exec??

SMT-EVAL's largest computational job was running all 45 of the evaluated solvers on all of the non-incremental benchmarks in the SMT library. The benchmarks belong to different {\em logics} and solvers are characterized by which logics they support. So for each logic, the evaluation executed the cross product of all benchmarks for that logic and all solvers (in all years) that support problems in that logic, for a total of 1\,663\,478 solver-benchmark combinations (called {\em job-pairs} in Star-Exec). This job took several months of wall time to run. We ran it in quarters, using the result of the first quarter to adjust our procedures and debug some of Star-Exec, before running the rest of the solver-benchmark job pairs. Because of a bug currently being corrected, the results of about 600 job-pairs were not present in the accumulated results. These were identified and rerun as an additional ``mop-up'' job.

\section{Evaluation results}
\label{Evaluations}

The evaluation team's observations on the questions it considered are presented in the following subsections; our overall conclusions are listed in Section~\ref{Conclusions}.  The raw data used as the basis for our observations, collected from Star-Exec, are all archived on the SMT-COMP website, at \url{http://sourceforge.net/p/smtcomp/code/HEAD/tree/trunk/smtcomp-web/2013/data} as 7z-compressed files.  (The largest is~25MB).

\subsection{Solver participation}
\label{Solvers}

The historically participating solvers are shown in Fig.~\ref{Fig:solvers}. There are a number of observations to be made about solver participation.

\begin{table}
\centering
\begin{tabular}{|l|ccccccccc|}
\hline
year                & 2005 & 2006 & 2007 & 2008 & 2009 & 2010 & 2011 & 2012 & 2013 \\ \hline
\# of participants  & 11   & 11   & 9    & 13   & 12   & 10   & 11   & 11   & 9   \\ \hline
\# dropping out     &      &  4   & 6    &  2   &  3   &  7   &  2   &  4   &      \\ \hline
\# new participants & (11) &  4   & 4    &  6   &  2   &  5   &  3   &  4   &      \\ \hline
\end{tabular}
\caption{Turnover in solver team participation}
\label{Table:participation}
\end{table}
\begin{itemize}
\item As shown in Table \ref{Table:participation}, the number of solvers participating each year has been fairly constant, ranging
from 9-13, with most years having 10-11 participants. The data for 2013 is an anomaly in two respects. The count of 9 only includes those solvers that were explicitly submitted new for 2013. Second, Abziz contributed a portfolio solver that makes an automatic choice, based on machine learning, of which among other existing solvers to apply to a benchmark using observed features of the benchmark. Abziz submitted two instances of his portfolio that used solvers from 2011; those instances of his portfolio solver are not counted in 
Table \ref{Table:participation} since they were not available in 2011, though they are included in Fig.~\ref{Fig:solvers}. In addition, three variations were contributed in 2012; we count just one participation unit in 
Table \ref{Table:participation} for 2012, though all three (and the two 2011 entries) were used in the evaluations described in this paper.

\item Though any team dropping out of future years' competitions is to be regretted, such turnover is to be expected: research projects and Ph.D. students move on to other topics. The year of the most drop-outs is 2010, when the benchmark format changed; not all teams could invest the effort to change their front-ends and to accommodate the new semantic features of SMT-LIB~v2. 
\item Accompanying the drop-outs is a roughly equal number of new participants each year; roughly 1/3 of the participants each year are new. One of the goals of an organized competition is to foster new participation, providing an venue in which it is easy to participate and self-evaluate against the state-of-the-art. Our observation is that this goal is being accomplished, despite the significant effort required to implement a reasonably competitive solver. 
\item Despite the turnover, some teams have participated regularly throughout the history of the competition: CVC3/4 and MathSat have participated since the beginning; Boolector, OpenSMT, veriT, and STP also have participated in most of the years since they began being involved.
\end{itemize}


\subsection{Progress in solver performance}
\label{Progress}

We can obtain a measure of the overall year-by-year improvement in solver performance by applying each year's solvers to all the benchmarks (within each logic), observing the best performance on each benchmark. That is, the data shows a {\em virtual best performer}, obtained by an all-knowing oracle choosing, for each benchmark, the solver that performs best on that benchmark.

Fig.~\ref{Fig:CumulativeTimes} shows the fraction of benchmarks completed (y-axis) within a given time (in seconds, on the x-axis); thus points toward the upper left are better (more completed in less time). The results for all logics are shown together. The four curves are the data for the four years from 2010 to 2013. The lowest (blue) curve is that for 2010. There is clearly significant progress made from 2010 to later years: the number of benchmarks completed within a given time is noticeably higher. The curves for 2011 and 2012 (red and green, respectively) are clearly above 2010, but are not significantly different from each other. The solvers for 2013 are uniformly above those for previous years, but not by a large amount.  

By this data, the improvements in raw SMT solver performance on the current set of benchmark problems has slowed down, though they may have improved by other measures.

\begin{figure}
\centering
\includegraphics[width=.8\textwidth]{CumulativeTimes2}
\caption{The fraction of benchmarks (y-axis) whose winning time is less than the given number of seconds (x-axis), by year.}
\label{Fig:CumulativeTimes}
\end{figure}

An alternate measure of solver progress is to count, for a given logic, what fraction of the benchmarks have a better time in a given year than in the previous year, by the best solver for each year. That data is shown in Fig.~\ref{Fig:bettertime}. Though most data points indicate that most benchmarks improved year to year, the progress is not uniform. The progress from 2010 to 2013 shows nearly all logics having improvement rates above 80\%, but there are definitely some laggards.

\begin{figure}
\centering
\begin{tabular}{|l|r|rrrr|}
\hline
\input{bettertime}
\hline
\end{tabular}
\caption{The fraction of benchmarks whose best time improved between the given years.}
\label{Fig:bettertime}
\end{figure}


\subsection{Repeatability of competitions}
\label{CompetitionRepeatability}

The SMT competition aims to be a repeatable measurement of relative performance of solvers. To that end the details of the competition, such as the rules and selection of benchmarks, are made public. In addition the solvers themselves are required to be made public after the competition.

However, there are aspects of the competition that are not deterministic. We measured two of those in this evaluation:
\begin{itemize}[noitemsep,nolistsep]
\item {\em The repeatability of the time a given solver takes on a given benchmark} (Section \ref{Accuracy}). This variation is a combination of two factors: any variation in execution and timing by the Star-Exec system itself, and any non-determinism in the execution of the solver.
\item {\em Variation caused by selection of benchmarks} (Section \ref{BenchmarkSelection}). \end{itemize}
 
 There is the additional risk that the set of benchmarks in SMT-LIB is not representative of any particular application area. We do not evaluate that risk here, but expect it to be quite likely to be the case.
 
\subsubsection{Accuracy of performance measurements}
\label{Accuracy}

To measure the repeatability of a given solver on a given benchmark, we produced a random selection of benchmark-solver pairs (job-pairs), selecting only among job-pairs that did not timeout and had a running time of at least 0.1 second. The random selection produced 10165 job-pairs (out of the total 1663478 job-pairs). These job-pairs were run 8 times on 8 different days. One such run took only a few hours. This resulted in 8 repetitions of each of the selected job-pairs. The data is archived with the other evaluation data on the SMTEVAL website. Star-Exec was lightly loaded on these days, so we did not measure any effect owing to interference with other uses of Star-Exec.

The 8 measured values for a job-pair were sorted and the first, median, and third quartile measured (as the average of sorted values 2 and 3, 4 and 5, and 6 and 7, respectively). We noted the following points:
\begin{itemize}[noitemsep,nolistsep]
\item Of the measured job-pairs, about 78\% had a Q3-Q1 spread less that 10\% of the median value. That is, roughly 50\% of the time repeated measurements will fall within about 10\% of each other.\footnote{We did not attempt to measure skew: the degree to which Q3 and Q1 are unequally distant from the median.}
\item 3.5\% of job-pairs showed a Q3-Q1 spread more than 50\% of the median value. 
\item Nine cases of the 10165 had a Q3 value more than double the median. These clearly showed multi-modal behavior among the eight data points for each benchmark-solver pair. Such behavior might be attributed to non-deterministic choices within the solver's search algorithms. 
\item We did not expand our study to determine whether the variation in performance was correlated to the choice of solver or to certain benchmarks.
\end{itemize}

An approximate conclusion from the above is that variation in timing itself plays a relatively small role in any variation in the competition. It presumably also averages out over\ many benchmarks and is not correlated with particular solvers. It might have a larger effect on benchmarks with short running times (but we did not quantify this.)

Any variation due to non-determinism within a solver we consider part of that solver's design. Thus it is possible that in some competitions a non-deterministic solver might do quite well, while in a repeat of the same competition that solver, making other internal random choices, would do poorly. 

\subsubsection{Accuracy of competitions}
\label{BenchmarkSelection}
The data we collected allows us to simulate various competition organizations. One such competition design would be, for each logic, to run all solvers for the given year on all benchmarks. The result of such an exercise is shown in Figs. \ref{Fig:virtual-competition-all}, \ref{Fig:virtual-competition-all2} and \ref{Fig:virtual-competition-all3} for the set of 9 solvers for the year 2013. By summing all the cpu times for each benchmark-solver combination, we obtain that such a competition would use about 443 days of cpu time; for this computation we consider all timeout and memory exhaustion results to have taken the full timeout value (1500 sec) of time. It is possible that in some cases a solver may exhaust memory prior to the timeout.

In previous competitions, the winner was determined as the solver that correctly solved the most benchmarks within the timeout limit (solvers producing incorrect results are disqualified). Ties among solvers solving the same number of benchmarks are resolved in favor of the solver taking the least amount of time produce its solutions. In the tables showing the results, for each logic, the results are reported in the order of a virtual winner.

\begin{figure}
\centering
\begin{tabular}{|p{.01in}rrl|}
\hline
\input{virtual-competition-all.tex}
\end{tabular}
\caption{(Part 1) Results of a virtual competition using the 2013 solvers on all benchmarks.}
\label{Fig:virtual-competition-all}
\end{figure}

\begin{figure}
\centering
\begin{tabular}{|p{.01in}rrl|}
\hline
\input{virtual-competition-all2.tex}
\end{tabular}
\caption{(Part 2) Results of a virtual competition using the 2013 solvers on all benchmarks.}
\label{Fig:virtual-competition-all2}
\end{figure}

\begin{figure}
\centering
\begin{tabular}{|p{.1in}rrl|}
\hline
\input{virtual-competition-all3.tex}
\end{tabular}
\caption{(Part 3) Results of a virtual competition using the 2013 solvers on all benchmarks.}
\label{Fig:virtual-competition-all3}
\end{figure}

However, past competitions have not used all benchmarks, but rather a random subset of the benchmarks. It is worth asking how susceptible the competition results are to the particular subset of benchmarks used for the competition. Typically, the choice is not completely random; rather, the distribution is constrained to have roughly equal representation of various categories of difficulty. For our evaluation here, we will determine the results of a virtual competition by simply randomly selecting different equal-sized subsets (ignoring difficulty measures), determining the competition results for each, and observing whether the competition results vary significantly depending on the subset.  Fig.~\ref{Fig:virtual-competition-some} shows the result of an experiment in which a virtual competition was executed on 1000 random subsets; we tallied the fraction of times that the winner changed and that the complete order of the finishers changed. (We limited our consideration to those logics with at least 100 benchmarks and with more than one solver.)

The results are quite instructive. In many logics, the competition winners change in only a small fraction of trials. 
However, in several logics, the fraction of trials in which the winner changes is quite significant, in some cases over 50\%. Inspection of the data reveals two contributing causes. 
\begin{itemize}
\item First, some logics have a relatively large number of benchmarks that are unsolved by the solver set within the timeout limit. Random selection of benchmarks can readily change the subset of unsolved benchmarks included in the virtual competition or change the balance of benchmarks solved by one solver vs. another. Since 'winning' the virtual competition is determined primarily by the number of benchmarks solved, rather than the time, any change in the relative number of solved benchmarks may change the order of winners. 

For example, the logic QF\_AUFBV has 14335 benchmarks. Of those, 520 are unsolved by Boolector and 543 are unsolved by SONOLAR (the winner and runner up of the virtual competition). Of these only 346 are unsolved by both, leaving 174 unsolved only by Boolector and 197 unsolved only by SONOLAR. It is thus reasonably probable that, in a selection of 10\% of the benchmarks, SONOLAR might have fewer unsolved benchmarks selected than Boolector, or vice versa. [TBD - validate with a calculation?]

\item Second, even when unsolved benchmarks are not a contributing factor, some pairs of solvers have total times that are close. Variations in time caused by slightly different choices of benchmarks might cause changes in winning order. The experiment described next explores this phenomenon further.

\end{itemize}

A second virtual competition can be run using only benchmarks that all competitors solved within the timeout period. This would not make a useful real competition because the result could be easily gamed: a solver could win by thoroughly optimizing performance on a few benchmarks and purposely not solving the remainder. However, in this evaluation, no solver has had the chance to do that; performing this evaluation allows comparing running times alone, without the additional factor of unsolved benchmarks.

Figs.~\ref{Fig:virtual-competition-solved}, \ref{Fig:virtual-competition-solved2}, and \ref{Fig:virtual-competition-solved3} show the result of such a competition, for the 9 2013 solvers; it uses all benchmarks that all the solvers registered for a given logic solve. In addition we performed the same experiment of 1000 virtual competitions each using 10\% of these benchmarks.  Fig.~\ref{Fig:virtual-competition-solved-some} shows the variation in winning order across these trials. The variation is even more than that shown in Fig.~\ref{Fig:virtual-competition-some}. To obtain some insight into this variation, we tabulate, in columns~2 and~3 of Fig.~\ref{Fig:virtual-competition-solved}ff, the mean and standard deviation\footnote{This is the population standard devation. To obtain the sample standard deviation scale these values by $\sqrt{0.999}$ } of the total solution time for each of the solvers in each logic. The standard deviations are substantial compared to the differences in mean times between competing solvers; even though the distributions of solving times are not necessarily gaussian, the data indicate a substantial probability that the order would change based solely on the random choice of benchmark subset.

\begin{figure}
\centering
\begin{tabular}{|lrr|rr|r|}
\hline
\input{virtual-competition-some.tex}
\end{tabular}
\caption{Results of virtual competitions using the 2013 solvers on random subsets of 10\% of the benchmarks.}
\label{Fig:virtual-competition-some}
\end{figure}

\begin{figure}
\centering
\begin{tabular}{|p{.01in}rrrl|}
\hline
\input{virtual-competition-solved.tex}
\end{tabular}
\caption{(Part 1) A virtual competition run only on benchmarks that all solvers solve.}
\label{Fig:virtual-competition-solved}
\end{figure}

\begin{figure}
\centering
\begin{tabular}{|p{.01in}rrrl|}
\hline
\input{virtual-competition-solved2.tex}
\end{tabular}
\caption{(Part 2) A virtual competition run only on benchmarks that all solvers solve.}
\label{Fig:virtual-competition-solved2}
\end{figure}

\begin{figure}
\centering
\begin{tabular}{|p{.01in}rrrl|}
\hline
\input{virtual-competition-solved3.tex}
\end{tabular}
\caption{(Part 3) A virtual competition run only on benchmarks that all solvers solve.}
\label{Fig:virtual-competition-solved3}
\end{figure}


\begin{figure}
\centering
\begin{tabular}{|lrr|rr|}
\hline
\input{virtual-competition-solved-some.tex}
\end{tabular}
\caption{Stability of competition results when run only on benchmarks solved by all solvers.}
\label{Fig:virtual-competition-solved-some}
\end{figure}

\subsection{Competitiveness of solvers}
\label{Competitiveness}

To assess competitiveness of the competitions and the set of solvers, we measured four quantities:
\begin{itemize}
\item {\em The ratio of the first to second place times on each benchmark in each logic.} The closer this quantity is to 1.0, the closer the race and the more the runner-up is challenging the leader. The ratio is averaged over all the benchmarks for each logic and year. The results are shown in Figs. \ref{Fig:runnerup1} and \ref{Fig:runnerup2}: for each logic and year that had more than one competitor, the mean, and 1st, median, and 3rd quartiles of the distribution of runner-up ratios are shown. 

A few logics, LRA and UFLRA, are very uncompetitive - the median case has the winner more than 10 times better than the runner-up. In a few others, such as QF\_IDL, QF\_NIA, QF\_NRA the winner is often less than half the time of the runner-up. But in most cases, the ratio is in the 50\%-100\% range. That is, the races are not neck-and-neck but the winners have only a modest lead over the next finisher. Note that the particular solver that is the winner varies from benchmark to benchmark.

\item {\em The degree to which the leader for a given benchmark changes from year to year.} Since all solvers are new each year, we placed solvers into families by the research group that produced them (e.g., the CVC family includes the CVC3 and CVC4 series of solvers). We counted it a {\em turnover} if the family of the winning solver changed from the previous year; we measured the fraction of benchmarks for a given logic and year that saw a turnover. The results are shown in 
Fig.~\ref{Fig:turnover}. There are many cases in which there is complete turnover from one year to the next; often this is because the previous solver family no longer participated or a new solver joined the competition and dominated the results. However, overall most
divisions see a more than 50\% turnover from year to year. We see this as indicative of reasonably robust competition.



\item {\em The distribution of winning solvers across the benchmarks within a logic.} A highly competitive environment would have each competing solver win a roughly equal fraction of the benchmarks; a non-competitive environment would have a single solver winning nearly all of the benchmarks. Our measure of competitiveness is a scaled entropy measure: if $f_i$ is the fraction of benchmarks won by solver $i$, and there are $N$ competing solvers, then the competitiveness measure is $ 2^{( - \sum_i f_i \log_2 f_i )} / N$. This quantity is $p/N$ if the wins are equally distributed among $p$ of the $N$ solvers ($f_i = 1/p$ for $p$ of the solvers and 0 for the other $N-p$). The results are shown in Figs. \ref{Fig:entropyA} and \ref{Fig:entropyB}; this data only includes benchmarks that were solved by a winner. The first entropy column gives the value of $ 2^{( - \sum_i f_i \log_2 f_i )}$, whose value is roughly the number of solvers over which the wins are distributed. The second entropy column is  $ 2^{( - \sum_i f_i \log_2 f_i )} / N$, which scales the first value between 
$1/N$ and 1.

There is a trivial case of one solver, with all wins distributed equally over the N=1 solvers and an unscaled `competitiveness' measure of 1.0; in all the combinations of logic and year, there are 18 such cases. Of the other year-logic combinations, note that in all but 11 (of the 70), despite any dominance by one solver, all competitors won at least one benchmark. The 'competitiveness' metric itself shows that in most cases there are approximately 2 solvers sharing the bulk of the wins, increasing to about 3 in a few cases that have many participants. The case of the most participants -- 11 solvers for QF\_BV in 2012 -- had a distribution among about 3.5 winning solvers. Thus, although nearly all solvers contribute something, performance is dominated by a few in nearly all competitive logics.

\item {\em SOTAC}. As a final measurement in this subcategory, we measured the {\em state of the art contribution} (SOTAC), as proposed  in \cite{webCASC}. This measures the uniqueness of the contribution of each solver. It does not consider the time taken to solve a benchmark, but just whether a solver solves a benchmark (within the timeout period). The contribution of a  benchmark to a solver's SOTAC is 0 if the solver does not solver the benchmarks and 1/(the number of solvers that solve that benchmark) if it does. Thus the maximum contribution is obtained when a solver is the only one to solve a benchmark. Table \ref{Tab:sotac} shows the computation for each of the solvers for the year 2013. The first column shows the sum of the SOTAC over all benchmarks in which a solver participated;
the second column is the average over all the benchmarks that that solver attempted (including ones that timed out), but not benchmarks in logics in which the solver did not participate; the third column\footnote{The third column corresponds to Sutcliffe's definition of SOTAC.} is the average over all benchmarks for which the solver was successful (did not time out).

The CVC4 and Z3 solvers have a high total SOTAC because they contribute to a broad range of logics. MiniSMT has a high average SOTAC because it does well at just a few logics. VeriT suffers on an overall average, but has a relatively high SOTAC averaged over those benchmarks that it solves.

\end{itemize}

\begin{figure}
\centering
\begin{tabular}{|ll|r|rrrr|}
\hline
\input{firstoversecond1.tex}
\end{tabular}
\caption{(Part 1) Runner-up: Ratio of winning time to runner-up time.}
\label{Fig:runnerup1}
\end{figure}

\begin{figure}
\centering
\begin{tabular}{|ll|r|rrrr|}
\hline
\input{firstoversecond2.tex}
\end{tabular}
\caption{(Part 2) Runner-up: Ratio of winning time to runner-up time.}
\label{Fig:runnerup2}
\end{figure}


\begin{figure}
\centering
\begin{tabular}{|l|rrr|p{.3in}|l|rrr|}
\hline
\input{turnover12.tex}
\hline
\end{tabular}
\vspace{.2in}
\includegraphics{turnover}
\caption{Turnover: Fraction of benchmarks for the given year and logic for which the winning solver is from a different solver family than the prior year.}
\label{Fig:turnover}
\end{figure}


\begin{figure}
\centering
\begin{tabular}{|ll|rrrrr|}
\hline
\input{windistA.tex}
\end{tabular}
\caption{(Part 1) Winner distribution: The degree to which winning is distributed among solvers vs. dominated by one solver.}
\label{Fig:entropyA}
\end{figure}

\begin{figure}
\centering
\begin{tabular}{|ll|rrrrr|}
\hline
\input{windistB.tex}
\end{tabular}
\caption{(Part 2) Winner distribution: The degree to which winning is distributed among solvers vs. dominated by one solver.}
\label{Fig:entropyB}
\end{figure}


\begin{table}
\centering
\begin{tabular}{|l|p{.9in}p{.7in}p{.7in}|}
\hline
\input{sotac.tex}
\hline
\end{tabular}
\caption{State of the art contribution from each of the 2013 solvers.}
\label{Tab:sotac}
\end{table}


\subsection{Lattice of Logics}
\label{Lattice}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{LogicLattice}
\caption{Lattice of SMT logics defined in SMT-LIB. {\tt QF\_ABV} is defined, but is often subsumed in {\tt QF\_AUFBV}, and is not usually listed separately in other tables in this report. {\tt Q} is not a logic by itself but is a characteristic indicating that quantified expressions are permitted in the logic.}
\label{Fig:lattice}
\end{figure}

SMT-LIB defines a number of theories and logics. The logics are a combination of theories with additional functions or logic symbols or restrictions on the vocabulary of the theories. For example, the {\tt QF\_IDL} logic uses the underlying {\tt Ints} theory, but restricts terms to equalities and inequalities between constants and simple differences of variables.
The combination of the {\tt Ints} theory and the {\tt Reals} theory adds functions that convert between integers and reals (among other things). Currently, no logic combines bit-vectors with integer or real arithmetic; if one did, it would be useful to provide conversions between numeric values and their corresponding (2's-complement or IEEE floating point) bit-vectors. Note that the {\tt QF\_ABV} logic is defined, but is often subsumed in {\tt QF\_AUFBV} and has no benchmarks of its own; it is not usually listed separately in other tables in this report.

Leaving aside difference logics as special cases of integer and real logics, there are essentially these mostly orthogonal characteristics underlying the theories and logics:
\begin{itemize}[noitemsep,nolistsep] %% these modifiers are not supported in all document classes
\item IA: integers 
\item RA: reals
\item N: non-linear arithmetic
\item A or AX: arrays
\item UF: uninterpreted functions
\item QF\_: quantification
\item BV: bit-vectors
\end{itemize}
This list of characteristics partially corresponds to the set of SMT-LIB theories. The exceptions are quantification and non-linearity, both of which designate lifting restrictions on the kinds of terms allowed in the resulting logic. (Since they do not correspond exactly to underlying SMT theories, the term {\em characteristics} is used in the discussion below.)

It is possible to define a logic with any powerset of these characteristics in an almost purely combinatorial sense. The only restrictions are
\begin{itemize}[noitemsep,nolistsep]
\item the interaction between bitvectors and integers or reals described above,
\item the interaction between integers and reals described above, and
\item one cannot have non-linearity without having either integers or reals,
\end{itemize}
Barring non-linearity without arithmetic and the useless empty set, there are then 111 combinations of characteristics (instead of the full 128). SMT-LIB has definitions and benchmarks for 20 of these logics (excluding QF\_RDL, QF\_IDL, and QF\_UFIDL). There are natural containment relationships among these, as shown in Fig.~\ref{Fig:lattice}. There are no combinations of bit-vectors with arithmetic.

The naming convention for logics is almost but not quite simply a combination of the letters indicated in the list above,
corresponding to the characteristics of the logic, with the letters listed in order by convention: {\tt [QF\_][A|AX][UF][BV][N|L][IA|RA|IRA]}. The non-uniformities are these:
\begin{itemize}[noitemsep,nolistsep]
\item quantification is indicated by {\em removing} a {\tt QF\_} prefix;
\item the absence of non-linearity ({\tt N}), i.e., linearity, is indicated by an {\tt L}, instead of by no designator; 
\item integer and real arithmetic are indicated by two letters ({\tt IA} and {\tt RA}) and their combination by {\tt IRA}; \item the {\tt A} used in the arithmetic designators could be ambiguous with the designator for arrays, except for position;
\item a logic with just arrays is named {\tt QF\_AX}, whereas in other combinations including arrays, just an {\tt A} instead of {\tt AX} is used.
\end{itemize}
For example, {\tt UFNIA} includes quantification (no {\tt QF\_} prefix), uninterpreted functions (UF), non-linearity (N), and integer arithmetic (IA), leaving out arrays, bit-vectors, and reals.

The ability to mix and match underlying characteristics makes it easy to define a logic with an (almost) arbitrary set of underlying characteristics. 
%In some cases, such (non-standard) additional logics have been used in papers [TBD-I wish I had a reference to where I've seen this.] 
It would be useful to standardize the naming convention along the lines of current use, perhaps with slight adjustments (e.g., renaming {\tt QF\_AX} to {\tt QF\_A} and removing the excess {\tt A} from {\tt IA} and {\tt RA}), in order to define precisely the logic implied by any combination of designators. This would
also make it easier to accurately characterize new benchmarks and to summarize the logics supported by a given solver.

The absence of many combinations appears to be simply a matter of ad hoc lack of research or non-existence of benchmarks. For example,
all five meaningful combinations of {\tt UF}, {\tt N} and {\tt RA} are present, but only four of the combinations of {\tt UF}, {\tt N} and {\tt IA}. Obviously there is no point to filling out all combinations, just for completeness. However,
benchmarks could come in any form, and it would be useful to be able to categorize them precisely. The collection of benchmarks should be driven by technical interest and application need. With the exception of some of the needed conversions (e.g., between bitvectors and numbers), there seems little technical impediment to combining characteristics once the corresponding underlying theories and decision procedures are implemented. That leaves application need as a driver; this is discussed further in section \ref{Applications}. It is also conceivable that some logics should be deemphasized, either because solvers have progressed to such an extent that the logic no longer poses interesting technical questions or because applications have little need for the logic.

\subsection{Coverage of Logics by Solvers}
\label{Coverage}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{LogicSupportBySolvers}
\caption{Support for each logic as stated by solver implementors. Darker highlighting identifies the 2013 versions of solvers.}
\label{Fig:solversupport}
\end{figure}

Each solver supports some subset of the SMT logics and the corresponding theories and characteristics described in the previous section. The set of logics supported by solvers (as stated by the solver teams) is shown in Fig.~\ref{Fig:solversupport}. CVC and Z3 support all or nearly all logics, with CVC4 just missing full support for non-linear arithmetic (it does have partial support); veriT and MathSat support a significant number of logics.

The row of numbers along the bottom of the figure state the number of current (2013) solvers that support a given logic. The logics with support from 4 or more solvers are highlighted. The characteristics most lacking are quantifiers and nonlinearity. Those with the most support are bitvectors, uninterpreted functions and arithmetic.

Note that the support indicated is that stated by the solver supplier. In some cases, a solver supports one logic but is not listed as supporting a subset of that logic. The general reason is that the subset, with restricted characteristics, allows for specific optimizations that are not implemented. The more general solver could act on problems of the
more restricted benchmarks but is not deemed competitive and thus was not formally entered into the competition for the restricted logic.

\subsection{Benchmarks}
\label{Benchmarks}

The number of benchmarks has successfully grown since SMT-LIB was established; there are now more than 100,000 in all of the logics combined. However, the logics differ significantly in the number of available benchmarks and in their overall difficulty.

Fig.~\ref{Fig:quintiles} has a column showing the number of benchmarks for each logic. The numbers vary considerably. Some logics, such as UFLRA, have just a very few benchmarks, while others have tens of thousands. SMT-LIB distinguishes four kinds of benchmarks:
\begin{itemize}[noitemsep,nolistsep]
\item {\em check} benchmarks are simple tasks that are designed to ensure that a solver has the basic functionality required for a division;
\item {\em industrial} benchmarks are generated from some application; ideally these are substantial examples showing real-world variation, but they be from toy applications running on a toy examples;
\item {\em crafted} benchmarks are hand-crafted to exercise a particular bit of functionality or technical challenge
\item {\em random} benchmarks are randomly generated from some distribution.
\end{itemize}
[TBD - need data and examples of these]
In addition, some benchmarks are {\em incremental}; that is, they contain more than one {\tt check-sat} command in a command script. This is relevant to interactive applications, but is not the main focus of the competition. In 2012, there were demonstration divisions on generating unsat cores and proof generation. These did not require special benchmarks, but do require different evaluation.

TBD- more on this aspect of competition design

Fig.~\ref{Fig:completion} shows the fraction of benchmarks (for each logic and year) that are solved within the 25 minute timeout period. For each combination of solver and year we report the fraction of benchmarks that are completed by all solvers and the fraction that are completed by at least one solver. In 2013, in about half of the logics, all benchmarks are completed by at least one solver within the timeout period (though not necessarily the same solver); in all but one logic (QF\_RDL), at least 95\% of the benchmarks are completed by at least one solver. The statistics for fraction of benchmarks completed by all solvers are not as high, since some of the solvers may be initial experimental versions and not tuned for competition. Even so, in more than half of the logics, at least 85\% of benchmarks were completed by all solvers.

Fig.~\ref{Fig:quintiles} is another view of benchmark difficulty. Here, for each logic, the distributions of winning times among the 2013 solvers are shown. In particular, selected
percentiles of each distribution are tabulated (the value for the {\em n}th percentile is the number of seconds for which that fraction of the benchmarks are completed by the winning solver for that benchmark). In all but three logics more than 80\% of the benchmarks for that logic take less than just a few seconds, if not less than a second. Only four of the logics have more than 5\% of their benchmarks that take more than the timeout period.
 
These results indicate that there is substantial room for more difficult benchmarks in almost all of the logics.

\begin{figure}
\centering
\begin{tabular}{|l|r|rrrr|rrrr|}
\hline
\input{completion}
\end{tabular}
\caption{The fraction of benchmarks completed by the solvers for the given year and logic.}
\label{Fig:completion}
\end{figure}

\begin{figure}
\centering
\begin{tabular}{|l|r|rrrr|}
\hline
\input{quintiles}
\hline
\end{tabular}
\caption{The distribution of winning benchmark times by logic (for 2013 solvers).}
\label{Fig:quintiles}
\end{figure}

\subsection{Application needs}
\label{Applications}

As part of its evaluation, the SMT-EVAL team solicited input on applications that use SMT-LIB. The response was not broad enough to be representative. In addition, the authors 
have encountered, by happenstance, enough users of SMT-LIB who are not active in the user community to indicate that there is likely a wide variety of uses that are not well-organized or well-represented in benchmarks on specific logics. A few application domains are fairly well-known, including software verification, scheduling optimization, and function and predicate synthesis. Software verification applications in particular will benefit from broader support for combinations of theories and better heuristics for quantification. 

In general, a better understanding of the variety of application needs is needed to target future development of SMT solvers and the SMT benchmark library.
 
%{\em Scheduling problems} typically use difference logics, either integer or real. A recent example is the Mihal's paper \cite{Mihal2013} at the SMT 2013 workshop.
%
%
%{\em Software verification} research uses SMT solvers to discharge verification conditions. In a purely model checking formalism, without user supplied axioms and specifications, a {\tt QF\_BV} or {\tt QF\_UFBV} logic would be sufficient (the latter is not defined, but would seem straightforward). However, in the formalism used by tools such as OpenJML \cite{Cok-2014-OpenJML,Cok-2011-OpenJML}, Dafny \cite{Leino:Dafny:LPAR16,Dafny2014} or Frama-C \cite{webframac}, both code and user-supplied specifications are translated to a logical form, expressed in SMT-LIB. In this case, the logic requires a broad range of underlying theories, including quantification. For this purpose solvers need to support all of the characteristics listed in subsection \ref{Lattice} and add support (theories and decision procedures) for strings, partially-ordered sets (to represent programming language types), and innovative theories and decision procedures to reason about uses of dynamic memory. Similarly SMT solvers are used underneath Ada tools 
%
%{\em Function synthesis}. A newer application of SMT constraint solving is to find functions or predicates satisfying given properties. Heizmann and Leike \cite{webLasso}
%use QF\_LRA and QF\_NRA logics to synthesize appropriate ranking functions for loops. 
%Similar work by the same authors uses interpolants to create or refine specifications; thus they use the interpolant-generation capability of some SMT solvers and the QF\_UFLIRA or AUFNIRA logics.

\section{Conclusions and Recommendations}
\label{Conclusions}

\subsection{Observations from the evaluation data}

The analyses described point to three principal conclusions and a number of observations.
\begin{itemize}
\item First, unsurprisingly, there is still a need for more and better benchmarks, despite the successful growth and current large quantity ($>$ 100,000) of benchmarks in SMT-LIB. Some logics have few benchmarks. In most logics, only a small fraction of benchmarks are significantly challenging (measured by time required to solve them).
\item Related to the above, a better sense of the application areas of SMT is needed. Such an analysis will drive solver research in application-oriented directions, provide focus on application-oriented logics, and guide application-relevant benchmark acquisition.
\item Finally, we discovered that using a random subset of benchmarks as the basis for the annual competition, together with inherent non-determinism in running solvers, significantly lessens the ability of a competition to determine `best' solvers at a given point in time. Simply rerunning a competition is quite likely to result in a different ordering of results. The best mitigation is to run as large a benchmark set as possible in a competition (and to work toward application-relevant benchmark sets).
\end{itemize}

Other observations are these:
\begin{itemize}
\item {\em Participation}. The number of participants is relatively stable (9-13) with an average turnover of 35\% (low of 16\%, high of 50\%) each year. There is also a core of continuing participants (about 1/3 of the total).
\item {\em Progress in solver performance}. Solver performance increased significantly from 2010 to 2013, but most of that improvement occurred from 2010 to 2011. There is improvement across nearly all logics, but some logics are lagging.
\item {\em Repeatability}. The repeatability of benchmarks is reduced by solver nondeterminism. Most importantly, however, competition repeatability is compromised by significant differences in performance of solvers on particular benchmarks, such that different random subsets of benchmarks will produce different assessments of solvers. The relevance of competitions is also threatened by the relevance of the total benchmark set.
\item {\em Competitiveness of solvers}. Within a logic nearly all solvers contribute something (e.g., a solution that no other solver produces). However, generally the winning times across benchmarks within a logic are obtained by 2-3 solvers that dominate that division. Comparing first-place times to second-place times shows that (a) in many logics the runner-up is quite close to the winner, but (b) in some logics, presumably with state-of-the-art tuning and technical algorithms, the winner far outstrips the other competitors. It is also relevant that most benchmarks change winner from one year to the next. Overall, we judge that there is a moderate degree of competitiveness among solvers - a few tend to dominate in any logic, but other solvers do also make their contribution.
\item The naming convention for logics could be more consistent.

\end{itemize}

\subsection{Recommendations for competitions and for SMT-LIB}

The experience of past competitions and this evaluation point to a number of ideas for future competitions. Most past policies have worked well and should be continued: openness, transparency, reproducibility, public submission of solvers. Other aspects could use improvement:
\begin{itemize}
\item To reduce potential variation caused by choice of benchmarks in a competition, a competition should include as many benchmarks as possible.
\item Invigorate the collection of benchmarks and promote a round of curating the existing benchmarks. This might include discontinuing or combining some logics.
\item Encourage the participation of new entrants through tool support and recognition. Some ideas are recognizing the best new entrant and promoting reference infrastructure for elements like parsing input files or standard reporting mechanisms.
\item Encourage broad participation by measuring performance in all logics, even when some logics are deemed more relevant than others.
\item Encourage broadening measures of performance beyond solution of single sat/unsat problems, by including, for example, determining unsat cores, measuring performance on incremental benchmarks, computing interpolants, or solvers that use multi-processing implementations.
\item Standardize the naming convention for logics.
\item There is a relative paucity of difficult benchmarks (those that are not solved within the timeout period). Perhaps this is because of progress in solver performance and in hardware speed. In either case, additional difficult benchmarks are needed.
\end{itemize}

\subsection{Future work}
The most important aspect of future work is to increase and improve benchmark quality, as described above. Thus we would encourage
\begin{itemize}
\item a broad survey of SMT application domains and their needs in solver support, both logics and functionality beyond determining satisfiability. This would also drive collection of relevant benchmarks.
\end{itemize}
There are also a number of questions that we posed but did not have the time or data to answer. 
\begin{itemize}
\item Are there interesting differences in performance between satisfiable and unsatisfiable benchmarks?
\item Do solvers support all of SMT-LIB~v2? Should the community encourage expansion to new capabilities (e.g., unsat cores, incremental, interpolants, model generation, proof generation)? Should the community define a core language that is used for the competition, with other aspects being optional?
\item Which logics should be retired or deemphasized, based on solver progress or application need?
\item Competitions primarily measure ability to solve benchmarks, with the time to do so a secondary criterion. However, in some applications it is important to discharge easy problems quickly. We did not assess this characteristic of the available solvers, but it would be worth doing so.
\item What variations in performance measurements occur because of interference from concurrent uses of Star-Exec?
\end{itemize}

\section{TODO}

% Boolector: QF_AX?
% SONOLAR: QF_AX. QF_UFBV
% TdW: QF_UF, QF_AX?
% VeriT: LRA?, QF_AX?, QF_BV?, QF_AUFBV?, QF_UFBV?
TBD:


- get information on kinds of benchmarks - crafted, application, etc.

- details on Star-Exec cluster; what was learned from initial use of Star-Exec

- text in png figures not very readable

- fix justification in sotac table

- how many other benchmarks

- this question in the outline is not really addressed: the degree to which benchmarks discriminate among solvers

- format problems: 
- what document class to use (where is this to be submitted);
- better legibility of figures
- unusual spacing on p.2/3
- fix bad boxes

- fix footer

\section*{Acknowledgments} 
 
The cost of executing the SMT competition is underwritten by the SMT
Workshop.  The evaluation used the Star-Exec cluster at the University
of Iowa.  This computation cluster replaces the SMT-Exec cluster used
for all previous competitions.  The cluster is supported by the
U.S.\ National Science Foundation under grant CNS-0551697.  Cok's
contribution is supported by GrammaTech, Inc.\ and in part by the
National Science Foundation under grant ACI-1314674.  Any opinions,
findings, and conclusions or recommendations expressed in this
material are those of the authors and do not necessarily reflect the
views of the National Science Foundation.
 
\bibliographystyle{plain}
\bibliography{SMTEVAL-2013,cok,sv}

\end{document}
