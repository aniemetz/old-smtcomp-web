%% Based on a TeXnicCenter-Template by Tino Weinkauf.

%%\documentclass[12pt,oneside]{report}
\documentclass[twosize,11pt]{article}

%% Language %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage[USenglish]{babel} %francais, polish, spanish, ...
%\usepackage[T1]{fontenc}
%\usepackage{textcomp}
%\usepackage[ansinew]{inputenc}
%\usepackage{makeidx}	  %% needed to create an index
\usepackage{jsat}
\usepackage{hyperref}   %% sets hyperlinks within a pdf
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{parskip}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{rotating}

%%\usepackage{lmodern} %Type1-font for non-english texts and characters


%% Packages for Graphics & Figures %%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage{graphicx} %%For loading graphic files
%\usepackage{subfig} %%Subfigures inside a figure
%\usepackage{tikz} %%Generate vector graphics from within LaTeX

%\usepackage{mathptmx} %% Times fonts, for math as well

%\newcommand{\tjark}[1]{\marginpar{\footnotesize{#1}}}
\newcommand{\comment}[2]{\begin{quote}\sc #1\marginpar{\textcolor{red}{$\ast^{\mbox{#2}}$}}\end{quote}}

\newcommand{\tjark}[1]{\comment{#1}{TW}}
\newcommand{\davidd}[1]{\comment{#1}{DD}}
\newcommand{\davidc}[1]{\comment{#1}{DC}}

\newcommand{\tjarkx}[1]{\tjark{#1}}

\newcommand{\rot}[1]{\begin{turn}{90}#1\end{turn}}

\jsatheading{number}{year}{startpage-endpage}
\ShortHeadings{The 2014 SMT Competition}{D. Cok et al.}
\firstpageno{1}

\begin{document}


\title{The 2014 SMT Competition}

\author{
\name David R. Cok (chair of organizing committee)
\email{dcok@grammatech.com} \\
\addr GrammaTech, Inc. \\
\AND
\name David D\'{e}harbe (co-organizer)
\email{david@dimap.ufrn.br} \\
\addr Federal University of Rio Grande do Norte, Brazil \\ 
\AND
\name Tjark Weber (co-organizer) \\
\addr Uppsala University, Sweden
\email{tjark.weber@it.uu.se}
}

\maketitle

\abstract{The 2014 SMT Competition was held in conjunction with the SMT workshop, affiliated with the CAV, IJCAR, and SAT conferences at FLoC 2014~\cite{FLoC2014}, at the Vienna Summer of Logic~\cite{VSL} in July 2014. Eighteen solvers participated from thirteen different research groups, across 34 different logic divisions. The competition was also part of the FLoC Olympic Games event, which gave combined visibility to 14 different competitions related to automated logic problem solving.} 
\tjarkx{A couple sentences on notable innovations or results}
The 2014 edition of the SMT Competition was executed for the first time on the Star-Exec~\cite{DBLP:conf/cade/StumpST14} logic solving service. Several records were broken: number of participating solvers, number of logic divisions and number of benchmarks.

\davidd{Comment on qualitative evolution of solvers?}
\keywords{SMT-solver, competition} \tjarkx{more keywords}

%\published{November 2014}{revision date}{publication date}

\section{Introduction}
\label{sec:intro}

The 2014 SMT Competition continued the series of annual competitions in SMT solver capability and performance that began in 2005. This is the 9th competition in the series, skipping only 2013; in that year an evaluation~\cite{it:2014-017} was performed, rather than a competition.

The competition is held to spur advances in
SMT solver implementations acting on benchmark formulas of practical interest. Public competitions are
a well-known means of stimulating advancement in software tools. For example, in automated
reasoning, the SAT and CASC competitions for propositional and first-order reasoning tools, respectively,
have spurred significant innovation in their fields~\cite{leberre+03,PSS02}.

The competition is sponsored by the SMT workshop, which was held in conjunction with the
CAV, IJCAR, and SAT conferences at FLoC 2014~\cite{FLoC2014}, at the Vienna Summer of Logic\cite{VSL} in July 2014.
Information about the winners
and results of the competition is summarized in this report and is available online at \url{www.smtcomp.org}; information
about previous years' competitions is also available at that website and in a published summary reports~\cite{springerlink:10.1007/s10817-012-9246-5,DBLP:conf/cade/CokGBD12,it:2014-017}.%[DD: done]\tjark{Cite 2012 and 2013 as well}

\davidd{Overview of the paper}

\section{Context of the competition}
\label{sec:context}

The SMT Competition is a competition among SMT solvers on a set of benchmark logic problems expressed in the SMT-LIB~\cite{BarST-RR-10} language. Each benchmark problem is a combination of definitions and logical assertions expressed with respect to an underlying logical theory and, perhaps, some constraints on the kinds of expressions in that theory. Each problem has a set of constant or function symbols; a solution to the problem is an assignment of each constant and function evalution to logical values in a way that satisfies all the problem's assertions. That is, the problem is to find a \textit{satisfying assignment} for the benchmark problem. For example, a problem might be simply a set of assertions in 
propositional logic, in terms of Boolean constants such as $P$, $Q$, and $R$.

As stated, the goal is the same as the SAT competition. The SMT benchmarks extend SAT by incorporating defined theories, such as the theory of arrays, or of undefined functions, or of arithmetic, or of bit-vectors. In addition, there may be constraints on the set of expressions allowed, such as only linear arithmetic, or only integer difference arithmetic. The theories also define \textit{sorts}, which make the theories a kind of typed first-order logic. Examples of sorts used in current theories are Boolean, Int, Real, bit-vectors of various lengths, and arrays with arbitrary sorts as index and value. Each combination of underlying theories and language constraints is a \textit{logic}.

The benchmark problems are expressed in the concrete syntax defined in the SMT-LIB (v2) standard \cite{BarST-RR-10}. This standard has been used since 2010, and a previous version was used prior to that. One of the goals of the SMT competition is to encourage use and tool implementations of the SMT-LIB standard.
\tjark{Give an example of a benchmark in SMT-LIB} \tjark{Provide information about the sizes of benchmarks}
\davidd{Shall we say something about command language?}

SMT (Satisfiability Modulo Theories) solvers are automated tools that seek to find a satisfying assignment for a given SMT-LIB problem, or to assure that the problem is \textit{unsatisfiable}.
Tools may not be able to solve a given problem, because, for example the tool exhausts available memory or time; a tool is permitted to answer \textit{unknown}. However, giving an incorrect answer (\textit{sat} instead of \textit{unsat}, or vice versa) is considered \textit{unsound} and a serious fault in the tool.

SMT differs from the CASC competition~\cite{PSS02} in directly addressing sorted logics. SMT also focuses on fragments of first-order logic that are \textit{decidable}. For example, a subset of the benchmarks are problems in an integer-difference logic, for which there are specific decision procedures. The competition among tools is to create very efficient implementations of a breadth of decision procedures. Accordingly, SMT solvers have historically also not handled logics with quantified expressions, though several currently now do so.

\section{The Competition Goals and Organization}
\label{sec:goals}

In planning the 2014 competition, the organizers' overall goal was to encourage breadth
in the capability of SMT solvers. SMTCOMP 2014 benefited from the evaluation that was performed in 2013 and the experience of the 2012 competition. As a result we established the following emphases:
\begin{itemize}
\item In 2012 we narrowed the competition to a smaller number of more significant logics. In response to feedback, in 2014 we reverted to the practice of evaluating solvers in all available divisions.
\item We ran the competition with all available benchmarks. To be eligible, a benchmark must be well-formed and have a known answer. We were able to use all benchmarks because we had nearly exclusive use of the Star-Exec cluster during the time we were executing SMT-COMP. This point is discussed further in section \ref{TBD}.
\item In 2012 some experimental tracks were held: parallel performance, unsat cores and proof generation. The participation in those was light. Since in 2014 we also had to migrate to using Star-Exec, we held only a main track and an application (incremental) track in the 2014 competition. The organizers still maintain the value of measuring the performance of solvers on new features such as model generation, unsat core determination and proof generation and recommend that they be reinstated in some future competition.

\tjarkx{More emphases?}

\end{itemize}



\section{Competition timeline}
\label{sec:timeline}

The preparation and execution of SMT-COMP 2014 took place over about 7 months, relying on the experience of previous competitions, the 2013 SMT Evaluation, and development activity on Star-Exec:
\begin{itemize}
\item July 2013 - Cok appointed as chair of the organizing committee
\item December 2013 - D\'{e}harbe and Weber appointed as co-coordinators
\item 21 January 2014 - call for applications and benchmarks
\item 15 May 2014 - deadline for new benchmarks; benchmarks were being corrected and curated throughout May and June, until the final solver deadline
\item 19 May 2014 - revised rules posted
\item 1 June 2014 - deadline for initial solver registration; final rules posted
\item 15 June 2014 - deadline for final solver registration
\item 16 June 2014 - computation begins
\item 22 June 2014- main track computation ends; official results posted on 6/27 
\item 22 June 2014 - deadline for application track solvers
\item 28 June 2014 - application track computation
\item 17 July 2014 - SMT Workshop at which results were announced
\item 20 July 2014 - FLoC awards ceremony \tjarkx{CHECK DATE - was it 20 or 21 or 22?}
\end{itemize}

\section{Competition procedure} 
\label{sec:procedure}


The full description of the 2014 SMT competition's rules is found in the rules document (\url{www.smtcomp.org/2014/rules14.pdf}). The document describes the procedures for determining benchmark difficulties, selecting benchmarks for competition, and judging the results.

\tjarkx{The rest of the material in this section includes material from 2012 and needs editing}

The competition's traditional `main' track tests a solver's ability to determine the satisfiability or unsatisfiability of a single problem (perhaps with multiple assertions) within a given logic. A second track tests the performance of multi-threaded solvers on similar problems.

The `application' or incremental track, introduced last year, tests a qualitatively different capability. Software verification tools often use SMT solvers as a back-end proof engine. These tools repeatedly invoke the solver with different, related satisfiability problems; the problems may have a substantially similar set of assertions, produced by the tool's adjusting, correcting, adding, or retracting assertions interactively; in batch mode different properties may be checked using substantially the same set of assertions.
The effect is that the solver must respond to a sequence of requests to assert or retract logical statements, check satisfiability, produce counterexamples, and so on. The application track tests a solver's performance in responding to such a sequence of commands, as produced by actual application problems. To implement the application track benchmarks, the 
competition uses a simulation engine that communicates with the solver just like an application (or a user at a keyboard) would, presenting each command and waiting for a response before presenting the next command; this mechanism (appropriately) prevents a solver from optimizing its effort based on knowing the entire sequence of commands all at once. A report
on the first (2011) year's application track and the overall design was presented by Griggio and Bruttomesso at the 
2012 COMPARE Workshop \cite{ag+rb+12}.

The benchmarks are each assigned a {\em difficulty}. The difficulty is based on how long it takes a group of solvers to produce a correct answer to the benchmark. For competition, benchmarks are selected, at random, from each difficulty category. 

The winning solver in each category is the one that produces the most correct answers in the least time. An additional change this year is that incorrect answers are a disqualifier: the organizers considered that solver technology has progressed sufficiently in capability and importance that incorrect answers should not be tolerated (a solver can always intentionally produce an answer of `unknown'). Each solver is given a fixed timeout period (this year the timeout was 20 minutes) in which to answer a benchmark. The winner is the solver that produces the most correct (non-{\em unknown}) answers and no incorrect answers; in the case of ties, the winner is the solver that took the least time to produce its correct answers.\footnote{There is an anomaly in this scoring system. Solvers A and B may produce the same correct answers, with A taking slightly less time to do so than B, and thus being the winner. Answers of {\em unknown} do not count towards correct answers, but the time taken also does not penalize the total time used. It may be the case the A takes a long time to determine an answer of {\em unknown} on some benchmarks, where as B can do so quickly. Thus B may be overall preferable in an application, even though A is the competition winner.}
In the unsat-core track, it is the size reduction of the core that is measured, rather than the number of correct answers.

\paragraph{The competition infrastructure.} The competition is executed on a cluster of machines at the University of Iowa, under the control of the SMT-EXEC software suite (cf. \url{www.smtexec.org}). This software suite has been used in past years as well. A new hardware and software infrastructure, Star-Exec (cf. \url{www.starexec.org}), is under development and was the subject of the Star-Exec workshop at IJCAR'12.

\paragraph{The SMTLIB language.} A competition based on benchmark problems needs a standard language in which to express those problems.
For SMTCOMP, that language is the SMT-LIB language (cf. \url{www.smtlib.org}, \cite{BarST-SMT-10} \cite{Cok-SMTLIBTutorial-2011}). 
In 2010, a significantly reworked version of the language was agreed upon.
This version 2 increased the flexibility and expressiveness of the language while also simplifying the syntax. 
It also includes a command language that improves the language's usefulness for interactive applications.
In particular, the standard specifies a typed (sorted), first-order logical language for terms and formulas, a language for specifying background logical theories and logics, and the command language. Some other tools that process SMT-LIBv2 are listed in the SMT-LIB web pages (cf. \url{http://www.smtlib.org/utilities.html}).

\section{Participants}
\label{sec:participants}

The competition registration includes information about each competing solver. In addition, some solver groups provided summaries of their solvers and their recent technical advances.
 Note that although one person is listed as the `submitter', there is generally a team of contributors behind each tool. Some teams submitted more than one tool. The 2014 participants were the following:
\begin{itemize}
\item 4Simp~\cite{TBD}- submitted by Trevor Hansen (U. Melbourne)
\item AbzizPortfolio~\cite{TBD} - two versions - submitted by Mohammed Adbul Aziz, U. Cairo. (This solver is unusual in that it is a portfolio solver: based on automated learning over benchmark characteristics, it chooses among other solvers to apply to the problem at hand.)
\item AProVe~\cite{AProVE2014} - submitted by Carsten Fuhs (UCL)
\item Boolector~\cite{TBD} - three versions - submitted by Armin Biere, Aina Niemetz, Mathias Preiner (Johnnes Kepler University)
\item CVC3 v. 2.4.3~\cite{TBD} - submitted by Morgan Deters (NYU)
\item CVC4 v. 1.4~\cite{BCD+11} - submitted by the ACSys Group (NYU)
\item Kleaver~\cite{TBD} - 2 versions - submitted by Hristina Palikareva, Cristian Cadar (Imperial College)
\item OpenSMT2~\cite{TBD} - submitted by Antti Hyv\"arinen \davidc{Affiliation?}
\item raSAT~\cite{TBD} - submitted by Tung Vu Xuan \davidc{Affiliation?}
\item SMTInterpol~\cite{DBLP:conf/spin/ChristHN12,DBLP:conf/spin/2012} - submitted by Jochen Hoenicke, J\"urgen Christ (U. Freiburg)
\item SONOLAR~\cite{TBD} - submitted by Florian Lapschies (U. Bremen)
\item STP-CryptoMiniSat4~\cite{DBLP:conf/cav/GaneshD07,DBLP:conf/cav/2007} - submitted by M\'at\'e So\'os \davidc{Affiliation?}, based on previous work by Trevor Hansen (U. Melbourne) and Vijay Ganesh (MIT)
\item veriT~\cite{TBD} - submitted by David D\'{e}harbe, Pascal Fontaine \davidc{Affiliation?}
\item Yices2~\cite{cav2014} - submitted by Bruno Dutertre (SRI)
\end{itemize}
\davidc{Need references for each of the above}
\davidc{Lists are inconsistent on STP2}

There were a few solvers that we hoped would be submitted but were not: Tiffany de Wintermonte was submitted in the past by Trevor Hansen, but could not be prepared in time for this competition; similarly SMT\_RAT was withdrawn because of last minute bugs; MathSat has been a frequent competitor in the past, but changes in priorities of the development team caused it not to compete in 2014; MiniSMT also was not able to be submitted; similarly, Z3, from Microsoft, though a strong tool has chosen not to take the time to prepare competition versions.

Other than those omissions, every competitive solver known to the organizers was represented. Indeed, the
 participation by solver teams was a record high in 2014. 

In addition to the competing solvers, the organizers, as in past competitions, included some publicly available historical solvers. These solvers are run in the competition for comparison, but are designated as demonstration only and are not eligible for any awards or designations of having won the competition. In result tables, these solvers are listed with their names in square brackets (e.g., [MathSat]). The organizers included current versions of MathSat and Z3. Also, during the competition, a bug-fix release of CVC4 (named CVC4-with-bugfix) was submitted and included as a demonstration only version (cf. section \ref{sec:floc}.

\begin{table}[t]
\centering
\begin{tabular}{|l|l|c|c|c|c|c|c|c|c|c|}
\hline
Solver & Affiliation & 2005 & 2006 & 2007 & 2008 & 2009 & 2010 & 2011 & 2012 & 2014 \\
\hline

4Simp                  & U. Melbourne 	&   &   &   &   &   &   &   &   & X  \\							
STP-CryptoMiniSat4     & TBD?           &   &   &   &   &   &   &   &   & X\\								
Kleaver	               & Imperial       &   &   &   &   &   &   &   &   & (2) \\								
raSAT  	               & TBD?           &   &   &   &   &   &   &   &   & X \\								
Tiffany de Wintermonte & U. Melbourne 	&   &   &   &   &   &   &   & X &  \\							
AbzizPortfolio         & U. Cairo       &   &   &   &   &   &   &   & X & (2)\\							
Boolector              & J.K. U.        &   &   &   & X & X &   & X & X & (3)\\
CVC/CVCLite/CVC3       & NYU, U. Iowa   & X & X & X & X & X & X & X & X & X \\
CVC4	                 & NYU, U. Iowa   &   &   &   &   &   & X & X & X & X \\
MathSat-HeavyBV        & U. Trento      &   &   &   &   &   &   &   & X &   \\								
MathSAT 3,4,5          & U. Trento, FBK & X & X & X & X & X & X & X & X &   \\
SMTInterpol            & U. Freiburg    &   &   &   &   &   &   & X & X & X \\
SONOLAR                & U. Bremen      &   &   &   &   &   & X & X & X & X \\
STP, STP2              & MIT            &   & X &   &   & X &   & X & X &   \\
AProVE NIA             & RWTH Aachen    &   &   &   &   &   & X & X &   & X \\
opensmt, opensmt2      & U. Lugano      &   &   &   & X & X & X & X &   & X \\
veriT                  & UFRN           &   &   &   &   & X & X & X &   & X \\
Z3                & Microsoft Research  &   &   & X & X &   &   & X &   &   \\
MiniSMT                & U. Innsbruck   &   &   &   &   &   & X &   &   &   \\	
simplifyingSTP         & U. Melbourne   &   &   &   &   &   & X &   &   &   \\	
test\_pmathsat         & FBK-IRST       &   &   &   &   &   & X &   &   &   \\	
barcelogic             & UPC            & X & X & X & X & X &   &   &   &   \\	
beaver                 & UC Berkeley   &   &   &   & X & X &   &   &   &   \\		
clsat                  & Washington U.  &   &   &   & X & X &   &   &   &   \\		
Sateen                 & U. Col-Boulder & X & X & X & X & X &   &   &   &   \\		
sword                  & U. Bremen      &   &   &   & X & X &   &   &   &   \\		
Yices, Yices2          & SRI            & X & X & X & X & X &   &   &   & X \\		
Spear                  &   &   &   & X & X &   &   &   &   \\		
Alt-Ergo               &   &   &   &   & X &   &   &   &   \\			
ArgoLib                &   &   &   & X &   &   &   &   &   \\				
Fx7                    &   &   &   & X &   &   &   &   &   \\				
Ario                   &   & X & X &   &   &   &   &   &   \\					
ExtSat                 &   &   & X &   &   &   &   &   &   \\					
HTP                    &   & X & X &   &   &   &   &   &   \\					
Jat                    &   &   & X &   &   &   &   &   &   \\					
NuSMV                  &   &   & X &   &   &   &   &   &   \\					
Sammy                  &   & X &   &   &   &   &   &   &   \\						
SBT                    &   & X &   &   &   &   &   &   &   \\						
Simplics               &   & X &   &   &   &   &   &   &   \\					
SVC	                   &   & X &   &   &   &   &   &   &   \\	
\hline					
\end{tabular}
\vspace{.2in}
\caption{History of solver participation}
\label{Table:participants}
\end{table}

\begin{table}
\centering
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
\hline
 & 2005 & 2006 & 2007 & 2008 & 2009 & 2010 & 2011 & 2012 & 2014\\
\hline
Participants                 & 12 & 12 & 9 & 13 & 12 & 10 & 11 & 11 & 18 \\
New in given year            & 12 &  4 & 4 &  6 &  2 &  6 &  1 &  4 & 5 \\
Continuing to the next year  &  8 &  6 & 7 & 10 &  4 &  7 &  7 &  6 &  \\
Not ever participating again &  4 &  5 & 2 &  2 &  6 &  3 &  4 &  4 &  \\ 
\hline
\end{tabular}
\vspace{.2in}
\caption{Changes in participation}
\label{Table:changes}
\end{table}

\paragraph{History.} Table \ref{Table:participants} shows the
 historical participation of each solver; note that sometimes versions and names change, or there are multiple related versions from the same team. 
Table \ref{} summarizes that data in numbers of continuing participants. Except for the record turnout in
 2014, there has been a steady 9-13 participants each year; each year there are an average of 4 new
 participants, about the same number of drop-outs, and an average of 7 continuing participants. 
The introduction in 2010 of SMT-LIBv2 as the standard language for benchmarks was a significant event. The new language required solvers to revise their front-ends and to add new capabilities.
As a result, some solvers did not continue participating, at least not immediately. However, the use of SMTLIBv2 also increased the expressiveness of benchmarks. Thus benchmarks representing the needs of industrial applications were able to be added; 
the application track of the competition was added to demonstrate this capability and the corresponding abilities of solvers.

\section{Competition divisions and benchmarks}
\label{sec:benchmarks}

The SMT-LIB benchmarks each belong to a specific logic. Each logic is one competition division. For each division we ran the solvers that entered that division on the benchmarks for that division as one event in the overall competition.

SMT-LIB currently defines 34 logics, shown in Table \ref{TBD}. The rightmost column shows the number of benchmarks for that logic in the SMT-LIB collection. Two considerations may make a benchmark ineligible to be used in a competition. First, the benchmark may not have a known result. For newly submitted benchmarks
we made an attempt to determine the correct result of the benchmark. the SMT-LIB coordinators require two 
different solvers to solve the problem and report the same result to admit the benchmark into the collection.
However, for benchmarks already in the collection we did not attempt to determine their answer. We did do this analysis after the competition was over (see section \ref{TBD}).

The second consideration is that the benchmark must be \textit{non-trivial}; a benchmark is deemed trivial if TBD.

The numbers of unknown, trivial, and remaining eligible benchmarks are also shown in Table \ref{TBD}. A few other comments are in order:
\begin{itemize}
\item We, with the SMT-LIB coordinators, performed a curation step on the benchmarks prior to the competition. We determined the actual logic to which a benchmark belonged, rather than the super-logic
to which it had previously been assigned. This resulted in an expansion of logics with benchmarks from 23 to 34, and also created a number of divisions with only a very few benchmarks.
\item Two divisions were not held because they had no eligible benchmarks.
\item The numbers of eligible, trivial, unknown, and total benchmarks vary widely from division to division. We will comment later on the need to review and improve the benchmark collection.
\end{itemize}

\begin{table}
\centering
\begin{tabular}{|l|r|r|r|r|r|}
\hline
       & \# of & Eligible & Unknown  & Trivial  & Total  \\
 Logic & solvers & benchmarks &  benchmarks &  benchmarks &  benchmarks \\
\hline
ALIA & 4 & 29 & 0 & 13 & 42 \\
AUFLIA & 4 & 4 & 0 & 0 & 4\\
AUFLIRA & 4 & 10791 & 168 & 9055 & 20014 \\
AUFNIRA & 4 & 564 & 443 & 488 & 1495 \\
BV &   & 0 & 191 & 0 & 191 \\
LIA & 4 & 46 & 0 & 0 & 46 \\
LRA & 4 & 171 & 364 & 86 & 621\\
NIA & 3 & 9 & 0 & 0 & 9\\
NRA & 3 & 3747 & 66 & 0 & 3813 \\
QF\_ABV & 9 & 6457 & 2906 & 5728 & 15091 \\
QF\_ALIA & 5 & 97 & 0 & 29 & 126 \\
QF\_AUFBV & 4 & 37 & 0 & 0 & 37 \\
QF\_AUFLIA & 6 & 610 & 0 & 399 & 1009\\
QF\_AX & 5 & 335 & 0 & 216 & 551 \\
QF\_BV & 11 & 2488 & 17728 & 12284 & 32500 \\
QF\_IDL & 4 & 1315 & 520 & 354 & 2189 \\
QF\_LIA & 7 & 4381 & 1192 & 568 & 6141 \\
QF\_LRA & 6 & 1343 & 167 & 172 & 1682\\ 
QF\_NIA & 4 & 8327 & 919 & 113 & 9359 \\
QF\_NRA & 4 & 10121 & 1382 & 37 & 11540 \\
QF\_RDL & 4 & 132 & 64 & 59 & 255 \\
QF\_UF & 7 & 4124 & 4 & 2522 & 6650 \\
QF\_UFBV & 4 & 31 & 0 & 0 & 31 \\
QF\_UFIDL & 4 & 311 & 0 & 130 & 441\\
QF\_UFLIA & 6 & 484 & 0 & 114 & 598 \\
QF\_UFLRA & 6 & 1176 & 73 & 381 & 1630 \\
QF\_UFNIA & 3 & 7 & 0 &0 & 7 \\
QF\_UFNRA & 3 & 32 & 10 & 1 & 43\\
UF & 4 & 2830 & 2911 & 7 & 5748 \\
UFBV & 0 & 0 & 191 & 0 & 191 \\
UFIDL & 3 & 49 & 12 & 19 & 80 \\
UFLIA & 4 & 5766 & 4680 & 1692 & 12138 \\
UFLRA & 4 & 25 & 0 & 0 & 25\\
UFNIA & 3 & 1587 & 1029 & 735 & 3351 \\
\hline
Total & 18 & 67426 & 35020 & 35202 & 137648 \\
\hline
\end{tabular}
\vspace{.2in}
\caption{Numbers of main-track benchmarks}
\label{Table:benchmarks}
\end{table}

A sizeable number, on the order of 30000, of benchmarks were added during the lead up to the competition. Some of these were submitted in previous years but never assessed and uploaded. Many others were supplied by solver developers (including some competitors). All of them went through an iterative curation process to be sure that they were syntactically valid, appropriate metadata was included, and a correct result was established. (Not all of these submissions were through the competition organizers.)

\tjarkx{Also want categories of benchmarks and numbers of sat or unsat}



Solvers could participate in any or all divisions at their team's discretion. Most solvers are designed for just one selected logic, but others are intended to be as broadly applicable as their developers have had time to implement. Table \ref{Table:logics} shows the participation of solvers in various divisions.

\begin{sidewaystable}
\centering
%%\small
\setlength\tabcolsep{3pt}
%%\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\begin{tabular}{|l|ccccc|cccc|cccccc|cccccc|ccccccc|cccccc|}
\hline
Solver & 	\rot{ALIA} & 	\rot{AUFLIA} & 	\rot{AUFLIRA} & 	\rot{AUFNIRA} & 	\rot{BV} & 	\rot{LIA} & 	\rot{LRA} & 	\rot{NIA} & 	\rot{NRA} & 	\rot{QF\_ABV} & 	\rot{QF\_ALIA} & 	\rot{QF\_AUFBV} & 	\rot{QF\_AUFLIA} & 	\rot{QF\_AX} & 	\rot{QF\_BV} & 	\rot{QF\_IDL} & 	\rot{QF\_LIA} & 	\rot{QF\_LRA} & 	\rot{QF\_NIA} & 	\rot{QF\_NRA} & 	\rot{QF\_RDL} & 	\rot{QF\_UF} & 	\rot{QF\_UFBV} & 	\rot{QF\_UFIDL} & 	\rot{QF\_UFLIA} & 	\rot{QF\_UFLRA} & 	\rot{QF\_UFNIA} & 	\rot{QF\_UFNRA} & 	\rot{UF} & 	\rot{UFBV} & 	\rot{UFIDL} & 	\rot{UFLIA} & 	\rot{UFLRA} & 	\rot{UFNIA} \\ 
\hline
4Simp & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	X & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 \\ 
Abziz & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	X & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 \\ 
Abziz2 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	X & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 \\ 
AProVE & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	X & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 \\ 
Boolector & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	X & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 \\ 
Boolector-d & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	X & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 \\ 
Boolector-j & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	X & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 \\ 
CVC3 & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	X & 	X & 	 & 	 & 	 & 	 & 	 & 	 & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X \\ 
CVC4 & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X \\ 
Kleaver-STP & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	X & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 \\ 
Kleaver-portfolio & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	X & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 \\ 
OpenSMT2 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	X & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 \\ 
raSAT & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	X & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 \\ 
SMTInterpol & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	X & 		X & 	X & 	 & 	 & 	X & 	X & 	 & 	 & 	 & 	X & 	 & 	 & 	X & 	X & 	 & 	 & 	 & 	 & 	 & 	 & 	 &  &	 \\ 
SONOLAR & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	X & 	 & 	 & 	 & 	 & 	X & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 \\ 
STP-Crypto... & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	X & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 \\ 
veriT & 	X & 	X & 	X & 	 & 	 & 	X & 	X & 	 & 	 & 	 & 	 & 	 & 	X & 			X & 	X & 	X & 	 & 	 & 	X & 	X & 		X & 	X & 	X & 	 & 	 & 	X & 	 & 	 & 	X & 	X &  & & &	 \\ 
Yices2 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	 & 	 & 	X & 	X & 	X & 	X & 	X & 	X & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 \\ 
\hline
{[}MathSAT5] & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	 & 	 & 	 & 	X & 	X & 	 & 	X & 	X & 	 & 	 & 	 & 	 & 	 & 	 & 	 & 	 \\ 
{[}Z3] & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X & 	X \\ 
\hline
\end{tabular}
\vspace{.2in}
\caption{Solver participation in logic divisions}
\label{Table:logics}
\end{sidewaystable}

\section{Main track results}
\label{sec:main-results}

Table \ref{Table:maintrack} shows a summary of the main track results. The detailed data for each 
division is on the competition website: \url{http://smtcomp.sourceforge.net/2014/results-toc.shtml}.
 
\begin{table}
\centering
\begin{tabular}{|l|r|r|l|l|}
\hline
Logic &	Solvers &	Benchmarks & Winner &	Order \\
\hline
ALIA & 		4 & 	29 & 	CVC4 & 	[Z3]; CVC4; veriT; CVC3; \\
AUFLIA & 	4 & 	4 & 	CVC4 & 	CVC4; [Z3]; CVC3; veriT; \\
AUFLIRA & 	4 & 	10791 & CVC4 & 	[Z3]; CVC4; CVC3; veriT; \\
AUFNIRA & 	4 & 	564 & 	CVC3 & 	[CVC4-with-bugfix]; [Z3]; CVC3; CVC4; \\
LIA & 		4 & 	46 & 	CVC4 & 	[Z3]; CVC4; CVC3; veriT; \\
LRA & 		4 & 	171 & 	CVC4 & 	CVC4; [Z3]; CVC3; veriT; \\
NIA & 		3 & 	9 & 	CVC4 & 	[Z3]; CVC4; CVC3; \\
NRA & 		3 & 	3747 & 	CVC4 & 	[Z3]; CVC4; CVC3; \\
QF\_ABV & 	9 & 	6457 & 	Boolector (justification) & 	Boolector (justification); Boolector (dual propagation); [MathSAT]; SONOLAR; CVC4; [Z3]; Yices2; Kleaver-STP; Kleaver-portfolio; \\
QF\_ALIA & 	5 & 	97 & 	Yices2 & 	Yices2; SMTInterpol; [Z3]; [MathSAT]; CVC4; \\
QF\_AUFBV & 	4 & 	37 & 	CVC4 & 	CVC4; Yices2; [Z3]; [MathSAT]; \\
QF\_AUFLIA & 	6 & 	610 & 	Yices2 & 	Yices2; [MathSAT]; [Z3]; SMTInterpol; CVC4; veriT; \\
QF\_AX & 	5 & 	335 & 	Yices2 & 	Yices2; [MathSAT]; [Z3]; CVC4; SMTInterpol; \\
QF\_BV & 	11 & 	2488 & 	Boolector & 	Boolector; STP-CryptoMiniSat4; [CVC4-with-bugfix]; [MathSAT]; [Z3]; CVC4; 4Simp; SONOLAR; Yices2; abziz\_min\_features; abziz\_all\_features; \\
QF\_IDL & 	4 & 	1315 & 	Yices2 & 	[Z3]; Yices2; CVC4; veriT; \\
QF\_LIA & 	7 & 	4381 & 	SMTInterpol & 	[CVC4-with-bugfix]; [MathSAT]; SMTInterpol; Yices2; [Z3]; veriT; CVC4; \\
QF\_LRA & 	6 & 	1343 & 	CVC4 & 	CVC4; Yices2; [MathSAT]; SMTInterpol; veriT; [Z3]; \\
QF\_NIA & 	4 & 	8327 & 	AProVE & 	[Z3]; AProVE; CVC3; CVC4; \\
QF\_NRA & 	4 & 	10121 & CVC3 & 	[Z3]; CVC3; CVC4; raSAT; \\
QF\_RDL & 	4 & 	132 & 	Yices2 & 	Yices2; [Z3]; veriT; CVC4; \\
QF\_UF & 	7 & 	4124 & 	Yices2 & 	Yices2; veriT; CVC4; OpenSMT2; [Z3]; [MathSAT]; SMTInterpol; \\
QF\_UFBV & 	4 & 	31 & 	Yices2 & 	Yices2; [Z3]; [MathSAT]; CVC4; \\
QF\_UFIDL & 	4 & 	311 & 	Yices2 & 	[Z3]; Yices2; CVC4; veriT; \\
QF\_UFLIA & 	6 & 	484 & 	Yices2 & 	[Z3]; Yices2; CVC4; SMTInterpol; [MathSAT]; veriT; \\
QF\_UFLRA & 	6 & 	1176 & 	Yices2 & 	[Z3]; Yices2; [MathSAT]; CVC4; SMTInterpol; veriT; \\
QF\_UFNIA & 	3 & 	7 & 	CVC4 & 	CVC4; [Z3]; CVC3; \\
QF\_UFNRA & 	3 & 	32 & 	CVC3 & 	[Z3]; CVC3; CVC4; \\
UF & 		4 & 	2830 & 	CVC4 & 	CVC4; [Z3]; CVC3; veriT; \\
UFIDL & 	3 & 	49 & 	CVC4 & 	[Z3]; CVC4; CVC3; \\
UFLIA & 	4 & 	5766 & 	CVC4 & 	CVC4; [Z3]; veriT; CVC3; \\
UFLRA & 	4 & 	25 & 	veriT & 	[Z3]; veriT; CVC3; CVC4; \\
UFNIA & 	3 & 	1587 & 	CVC4 & 	[Z3]; CVC4; CVC3; \\
\hline
\end{tabular}
\vspace{.2in}
\caption{Main track results}
\label{Table:maintrack}
\end{table}


\tjarkx{Include table of main track results - in preparation}

\tjarkx{Include discussion of main track results - what points to make?}


\section{Application track results}
\label{sec:application-results}

Most solvers are only concerned with raw performance on single benchmarks. However, an important application area for SMT solvers requires what the SMT-LIB standard calls 'incremental' operation.
In this mode, a user or some application interacts with the solver repeatedly, issuing various commands to define a problem, check for satisfiability, inspect resulting counterexample models, adjust the problem, and so on. An example use case is a tool that allows a user to author formal specifications in conjunction with software. The tool would
check using a back-end SMT solver whether the specifications are consistent with the code. If not, it might
supply a counterexample that could be inspected in conjunction with the source code. As the user edits the source or the specifications, the problem presented to the SMT solver is modified. 

The application track of the competition presents to the solver a series of SMT-LIB commands through a driver program; the solver replies to the driver with a 
response to each command. The driver presents commands only one at a time to emulate a realistic environment and so that the solver cannot `work ahead'. The driver also measures the time taken for each response. Note that the time measured includes the response time to every command, not just to the satisfiability-checking commands.

Four solvers participated in this competition track: CVC3, CVC4, SMTInterpol, and Yices2; Z3 was added as a demonstration-only historical comparison.

\tjarkx{Include table of application track results - need to reduce the data!!!}

\tjarkx{Include discussion of application track results - what points to make?}


\section{FLoC Olympic games}
\label{sec:floc}

The main track and the application track described in previous sections are staples of recent SMT competitions. The 2014 edition was unique in also being associated with the FLoC Olympic Games~\cite{TBD}.
This association was positive in providing a platform, along with the other competitions, to present the
rationale, methodology and results of the SMT competition to a wider audience than just the SMT community.

One additional aspect that resulted from the Olympic Games was the awarding of three medals to the three ``winners" of the competition. Since SMT-COMP is organized into many separate divisions, with winners determined in each division independently, the organizers had to determine how to award three global prizes.
The metrics for doing so were the subject of significant discussion both before and after the competition.
The metrics were decided by the organizers before the competition began (and before the deadline for solver registration) and were maintained unchanged after the competition.

The organizers chose to award the bronze medal for the best performance in a single division. We chose  the QF\_BV division for this medal because it is significant to applications and because it traditionally has received the most solver submissions. Indeed in 2014, there were 11 participants. Determining the winner was straightforward: we used the same metric as is used for each division - the most problems solved without errors, with ties broken by speed of solution. The Boolector~\cite{TBD} solver won this division and the bronze medal. The results for all participating solvers are shown in Table \ref{Table:bronze}.

The organizers chose to award the silver and gold medals for best performance across the most divisions. Thus we needed a metric that combined the results across divisions. We considered two metrics. For a given solver, let 
\begin{itemize}
\item $e_i$ be the number of benchmarks in division $i$ for which an incorrect result was produced
\item $c_i$ be the number of benchmarks in division $i$ solved correctly
\item $t_i$ be the total time to solve the benchmarks in division $i$ that were solved correctly
\item $N_i$ be the total number of benchmarks in division $i$.
\end{itemize}
The normal metric for a division is that the winning solver is the one with the
smallest value of $e_i$, the largest value of $c_i$ and then the smallest value of $t_i$, for each division taken separately; that is, the metric is a lexicographic ordering by smallest value of $[ e_i, -c_i, t_i ]$.
The the two global metrics we considered are
\begin{itemize}
\item Metric A: The winning solver is the one with the smallest value of $\sum_i e_i$, then the largest value of $\sum_i c_i/N_i \log N_i$, then the smallest value of $sum_i t_i \log N_i$, where for a given solver the sums are over all competitive divisions in which that solver participated
\item Metric B: The winning solver is the one with the smallest value of $\sum_i e_i$, then the largest value of $\sum_i (e_i == 0 ? c_i/N_i \log N_i : - \log N_i )$, then the smallest value of $sum_i t_i \log N_i$ 
\end{itemize}
Note that incorrect results are very rare, but do occur; for almost all solvers and divisions the value of $e_i$ is 0 and the competition hinges on the values of $c_i$. The speed of the solver is important in two ways. First, if the solver is slow, it will time out before a solution is found and thus the value of $c_i$ will be lower. Second, if there is a tie in the number of errors and correctly solved problems, the total time taken on the correctly solved problems is used as the tie-breaker; this is a rare occurrence but does happen if, for example, all the benchmark problems in a division are solvable within the time limit.

Only \textit{competitive divisions} were included in the scoring. For determining medals, a division is competitive if there are at least two officially registered, participating solvers \textit{from different teams}. This prevents a team from gaming the scoring by submitting multiple solvers to divisions in which no one else is participating. This criterion excluded a number of divisions from medal scoring: AUFNIRA, BV, NIA, NRA, QF\_UFNIA, QF\_UFNRA, UFBV, UFIDL, UFNIA.

The log scaling of the scores for each division is a somewhat arbitrary means to adjust the scores for the wide variety of numbers of benchmarks. It seemed a reasonable compromise between linearly combining numbers of benchmarks, which would have overweighted large divisions (in the organizers' view), and simply summing the fraction of benchmarks solved correctly, which would have overweighted small divisions.

These two metrics differ in how errors are treated. If a solver has no errors, it is always better off to participate in as many divisions as possible. However, an error in a division penalizes a solver so that it would be better not to have participated in the division; hence the organizers ruled that once the competition had started, a solver could not be withdrawn from a division in which it was registered.

The penalty for an error is globally significant in Metric A: a single error in one division out of many would put the solver behind any other solver with no errors, even if that solver participated in just one division. The penalty for an error is more local for Metric B: the error results in a large negative score for that division. Both metrics satisfy the criterion of putting heavy weight on correctness of solvers.
The organizers published the choice of Metric A as the metric for the Olympic Games gold and silver medals in the rules prior to the beginning of the competition with no objection at the time.

Though all solvers were scored for the medal metrics, five solvers participated in more than two divisions and were the most competitive during the course of the competition. The final results are shown in Table \ref{Table:medals}. The choice of metric did have a significant effect on the result. CVC4 and Yices2 participated in the most divisions, solved the most problems, and did so the most efficiently. However, Yices2 had a crash on one problem in QF\_ABV, but had emitted an erroneous answer prior to the crash (a simple crash without an answer is scored the same as an `unknown' response, marked neither wrong nor correct). CVC4 had bugs that affected AUFNIRA, which was not a competitive division, and QF\_LIA,
which was competitive. Consequently, by the competition metric, these two otherwise leading solvers placed much further back in the pack.

The resulting winning teams, veriT and SMTInterpol, put an appeal to the organizers to use Metric B instead, arguing that (a) CVC4 and Yices2 were clearly the more capable solvers and (b) there were known bugs in the winning solvers as well, which, simply by good fortune, were not triggered by the competition benchmarks. However, after public comment acknowledging the good will of the winners, the appeal was not accepted by the organizers. The medal ceremony did highlight the differing contributions of all four teams as well as those of the bronze medal winner. Note that the discovered bugs were promptly fixed. In fact, CVC4 submitted an additional demonstration-only version, named CVC4-with-bugfix in the result tables, which the organizers ran in conjunction with the rest of the competition.

\begin{table}
\centering
\begin{tabular}{|l|r|rr|r|}
\hline
Solver  & Competitive & Metric A & & Metric B \\
 & Divisions & Weighted errors & Weighted solved & \\
\hline
veriT & 17 &	0.000 	& 	25.325 & 25.325 \\
SMTInterpol & 8 &	0.000 	& 	22.831 & 22.831 \\
CVC3 & 10	& 	0.000 	& 	9.618 & 9.618 \\
SONOLAR 	& 2	&  	0.000 	& 	5.978 & 	5.978\\
AProVE 		& 1 & 	0.000 	& 	3.776	& 	3.776\\
Boolector (justification) & 1	&  	0.000 	& 	3.758	& 	3.758\\
Boolector (dual propagation) & 1		&  	0.000 	& 	3.755 & 	3.755\\
OpenSMT2 	& 1	&  	0.000 &	3.582 &	3.582\\
Boolector & 1		&  	0.000 &	3.058  &	3.058 \\
STP-CryptoMiniSat4 	& 1	&  	0.000 &	2.859 &	2.859\\
4Simp 	& 1	& 	0.000 	& 	2.468  	& 	2.468 \\
raSAT 	& 1	& 	0.000 	& 	0.000 	& 	0.000 \\
Yices2 	& 15	&  	3.810 	& 	38.624 & 31.059 \\
CVC4 		& 25 &  	7.283 	& 	54.152 & 43.509 \\
abziz\_min\_features 	& 1	& 	30.563 	& 	2.548 & -30.563 \\
abziz\_all\_features 	& 1	&  	30.563 	& 	2.403 & -30.563 \\
Kleaver-STP 	& 1	&  	213.362 	& 	3.103 & -213.362 \\
Kleaver-portfolio & 1		&  	346.713 	& 	3.073  & -346.713 \\
\hline
\end{tabular}
\vspace{.2in}
\caption{Gold and silver medal competition, in winning order by Metric A. By Metric B the order is the same except that CVC4 and Yices2 are in first and second place.}
\label{Table:medals}
\end{table}

\begin{table}
\centering
\begin{tabular}{|l|rrr|}
\hline
 Solver & Errors & Solved & Time (sec)\\
\hline
Boolector &	0  &		2361  &		138077.59 \\
STP-CryptoMiniSat4 & 0  &		2283  &		190660.82 	\\
{[}CVC4-with-bugfix] &	0  &		2237 	 &		139205.24 \\
{[}MathSAT]  &	0  &		2199 	 &		262349.39 \\
{[}Z3]  & 	0  &		2180 	 &		214087.66 	\\
CVC4  &	0  &		2166 	 &		87954.62 	\\
4Simp &	0 &	2121 	 &		187966.86 \\
SONOLAR &	0  &		2026  &	 	174134.49 \\
Yices2 &	0  &		1770  &	 	159991.55 \\
abziz\_min\_features &	9  &		2155 	 &	 	134385.22 \\
abziz\_all\_features &	9  &		2093 	 &	 	122540.04 \\
\hline
\end{tabular}
\vspace{.2in}
\caption{Bronze medal competition (QF\_BV division - 2488 benchmarks), in winning order}
\label{Table:bronze}
\end{table}

\tjarkx{In the bronze medal table, CVC4-with-bugfix, MathSat, and Z3 should  be in square brackets, but I keep getting errors for that in table environment}


\section{Concluding observations}
\label{sec:conclusion}

- 2015 competition is planned (where when?)

- renewed participation

- Star-Exec worked well

- benchmark resolution

- more focus on solving small problems quickly

- still need breadth of solvers

- using all benchmarks may not be the best design - over emphasis on some problem types

- still need a focus on applications; more representative benchmarks; more benchmarks needed in many logics

- spawning of SL\_COMP

- Metrics: include the time for non-success computations

- Difficulty of preparing new submissions

THIS IS ALL FROM 2012

\paragraph{Measuring improvement.}


One of the goals of a repeated competition is to prod improvement from year to year. Hence one would like measures of such improvement. We currently measure such improvement only in a limited sense in that we only make year-to-year comparisons on selected benchmarks. Such measures are imperfect because the set of solvers, the competition benchmarks and the difficulty ratings all change from year to year. Recall that the difficulty ratings are calculated by running last years' solvers on the benchmarks. In general, if solvers are improving, the difficulty ratings should decrease. Unfortunately, the historical ratings were not kept with sufficient precision to enable such a comparison. 

One source of comparison noise is the variation in the benchmarks. The benchmarks used in a given year are a sampling from the set of all SMT-LIB benchmarks. It is not known (and is a planned study once Star-Exec is ready) how much the sampling variation would change the performance results of individual solvers.\footnote{There is also the question of the degree to which the full benchmark suite is a representative sample of the universe of `interesting' problems -- which is another way of asking about the similarity of SMT-LIB benchmarks to any particular application space.} Another source of noise is variation in the set of solvers used; in particular, in some categories, last year's winner was not an entrant in 2012.

However, a head-to-head comparison of this year's winner vs. last year's winner, in each category, on the selected 2012 benchmarks is a straightforward summary of the competition results. Such comparisons are shown in Fig. \ref{Comparisons}.

We can roughly and informally compare the 2012 results to 2011 in these observations.
\begin{itemize}
\item The winning entrant in only a few divisions surpassed last year's winner in that division measured by the overall number of benchmarks solved. This only indicates that the leader still leads; other solvers may well be improving.
\item The scatterplots in Fig. \ref{Comparisons} show more detail. Division QF\_AUFBV shows clear improvement with QF\_BV, QF\_LRA and QF\_LIA showing mixed results. The non-competitive divisions (AUFLIA+/-p and QF\_IDL) show CVC4 still catching up to Z3.
\item In most categories even last year's leader did less well on this year's benchmarks (solving fewer problems). This indicates that the randomly chosen benchmarks were harder this year - that is, the benchmark difficulty ratings have become smaller, so the selection mechanism chose more hard benchmarks. This indicates at least that the 2011 solvers were better than 2010, causing the difficulty ratings on individual benchmarks to decrease.
\end{itemize}

\paragraph{Difficulty of preparing entrants.} In the post-competition discussion, solver submitters discussed the difficulty of preparing solvers for competition. This discussion highlighted a tension in competitions: the trade-off between developing new research ideas and engineering the tools. An academic researcher is rewarded for well-demonstrated new ideas in solver algorithms. However, producing a well-performing solver requires a significant amount of engineering that does not necessarily contribute to publishable papers or theses. 
But, a person interested in using a solver in an application area of interest will definitely value a robust tool with good time and space performance that scales to industrial-size problems. The SMTCOMP competition purposely emphasizes correctness and performance; thus engineering is essential.

\paragraph{Move to Star-Exec.} It is the intention of the SMT community to host future competitions on the new Star-Exec infrastructure (cf. \url{www.starexec.org}). Star-Exec was rolled out at the Star-Exec workshop at IJCAR'12; a number of organizers of competitions (including SMTCOMP) were present and had the opportunity to experiment with and comment on its 
design, architecture, and implementation. The SMTCOMP organizing committee will be working with Star-Exec to port materials from the SMT-Exec infrastructure to Star-Exec in preparation for the next competition.


\paragraph{The next competition.} The SMT business meeting made a tentative decision that the next SMT workshop would be held in conjunction with SAT 2013. The SMT competition will continue to be held in conjunction with the SMT workshop. However, there was some interest in holding the competition just every other year. On the other hand, the Star-Exec infrastructure is nearly ready to deploy; it would be advantageous to be exercising that infrastructure during 2012-2013 in preparation for a 2013 competition. The informal consensus, pending a decision by the SMT steering committee, is to hold a competition in 2013 on the Star-Exec framework, even if it is simply a rerun of solvers submitted for 2012.

\paragraph{SMTCOMP and CASC.}

The emphasis of SMT is solving constraint problems consisting of ground formulae built on background theories and using known or new decision procedures. Though some benchmarks use quantifiers, SMT solvers in general are not well-suited to problems with quantification. In contrast, the CASC competition, associated with CADE (Conference on Automated Deduction) uses the TPTP problem set; these problems are typically expressed as first-order formulae, perhaps with built-in equality or arithmetic (and in a different syntactic format). Thus CASC problems are quantifier-centric and construct proofs of theorems heuristically.

At IJCAR'12 the organizers of the two competitions (David Cok and Geoff Sutcliffe) discussed ways of bringing the advantages and strengths of each community to the other. The driving motivation is that many application problems are best expressed using SMT-like theories but with quantification. A first step may be to find interesting problems at the 
intersection of the two domains, express them in the two different problem formats, and apply tools from each domain, comparing the results. A set of problems the organizers are considering is CASC's TFA division --- typed first-order theorems with Arithmetic. These would correspond variously to SMT-LIB's AUFLIRA and AUFNIRA logics or more specialized subsets of those (and without explicit arrays). Such a set of common problems would allow a direct comparison of ATP and SMT system's capabilities.
   
\section{Recommendations for future competitions}

The SMT steering committee has decided that SMT-COMP 2015 will be held in association with the SMT Workshop, which itself will be affiliated with \tjarkx{WHO} in \tjarkx{WHERE} from \tjarkx{WHEN}.
The Workshop is being organized by \tjarkx{WHO}; the organizers of the competition are not yet appointed.

Based on the experience of 2014, the 2014 organizers have the following recommendations or topics for consideration for future competitions.
\begin{itemize}
\item
\end{itemize}
\tjarkx{More recommendations}
\section*{Acknowledgments} 
\begin{itemize}
\item The organizers were supported by their respective institutions (GrammaTech, Federal University of Rio Grande do Norte, Brazil and Uppsala University, Sweden respectively). 

\item In addition Cok received partial support from the U.S. nation Science Foundation
under grant ACI-1314674.

\item Clark Barrett and Morgan Deters assisted with some aspects of benchmark preparation,
in their roles as SMTLIB coordinators.

\item Aaron Stump and the StarExec support team were essential in keeping the competition cluster running;
in this first large-scale, public use of the cluster, numerous small details needed correction and were corrected promptly.

\item The StarExec cluster is supported by 
U.S.\ National Science Foundation under grants \#1058748 and \#1058925.


\item The cost of executing the SMT competition is underwritten by the SMT Workshop. 
\end{itemize}

Any opinions,
findings, and conclusions or recommendations expressed in this
material are those of the authors and do not necessarily reflect the
views of the National Science Foundation.

\bibliographystyle{plainbv}
\bibliography{SMTCOMP}

\end{document}

\thebibliography

