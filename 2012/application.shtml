<!--#set var="title" value="Application track resources"
--><!--#include virtual="smt-comp-prelude.shtml" -->

<p>
These benchmarks and tools are subject to change until their respective freeze
dates, as documented in the <a href="rules.shtml">SMT-COMP rules</a>.
</p>

<h2>This page lists the 2011 benchmarks; 2012 benchmarks will be listed when they are received and processed.</h2>

<h2>Benchmarks</h2>
<ul>
  <li><a href="http://www.smtcomp.org/2011/application/blast_simplify_calls_QF_UFLIA.tar">QF_UFLIA benchmarks, from Blast (128MB)</a><br/>
 These are just logs of the calls to the Simplify theorem prover, used
 by Blast when trying to model check some C programs (the name of the
 program is reflected in the name of the benchmark -- original Simplify traces
 can be made available for those who are interested)<br/>
      <a href="http://www.smtcomp.org/2011/application/blast_simplify_calls_QF_UFLIA_results.tar.gz">Results</a>
  <li><a href="http://www.smtcomp.org/2011/application/hybrid_networks_QF_LRA.tar">QF_LRA BMC and k-Induction problems on networks of hybrid automata, from NuSMV (161MB)</a><br/>
 These are benchmarks for hybrid automata, generated by unrolling the NuSMV models and checking
 them with BMC or k-Induction<br/>
      <a href="http://www.smtcomp.org/2011/application/hybrid_networks_QF_LRA_results.tar.gz">Results</a></li>
  <li><a href="http://www.smtcomp.org/2011/application/kratos_systemC_swmc_QF_BV.tar">QF_BV BMC and k-Induction problems on SystemC designs, from NuSMV (134MB)</a><br/>
 Unrollings of translation of some SystemC programs into NuSMV. The programs were those used, e.g., in the FMCAD'10 paper:
 <i>Verifying SystemC: a Software Model Checking Approach</i> by Alessandro Cimatti, Andrea Micheli, Iman Narasamdya and Marco Roveri
 The two sets are essentially the same, except for the different logic
 used.<br/>
      <a href="http://www.smtcomp.org/2011/application/kratos_systemC_swmc_QF_BV_results.tar.gz">Results</a></li>
  <li><a href="http://www.smtcomp.org/2011/application/kratos_systemC_swmc_QF_LIA.tar">QF_LIA BMC and k-Induction problems on SystemC designs, from NuSMV (346MB)</a><br/>
 Unrollings of translation of some SystemC programs into
 NuSMV. The programs were those used, e.g., in the FMCAD'10 paper:
 <i>Verifying SystemC: a Software Model Checking Approach</i> by
 Alessandro Cimatti, Andrea Micheli, Iman Narasamdya and Marco Roveri
 The two sets are essentially the same, except for the different logic
 used.<br/>
      <a href="http://www.smtcomp.org/2011/application/kratos_systemC_swmc_QF_LIA_results.tar.gz">Results</a></li>
  <li><a href="http://www.smtcomp.org/2011/application/lustre_QF_LIA.tar">QF_LIA BMC and k-Induction problems on Lustre programs, from NuSMV (172MB)</a><br/>
 These were obtained from subset of the Lustre models also used for
 the KIND set, except that they were generated from a NuSMV version of
 the Lustre programs, by NuSMV itself.<br/>
      <a href="http://www.smtcomp.org/2011/application/lustre_QF_LIA_results.tar.gz">Results</a></li>
  <li><a href="http://www.smtcomp.org/2011/application/kind_QF_UFLIA_20110606.tgz">QF_UFLIA traces from KIND, postprocessed (for competition, 128MB).</a><br/>
 These benchmarks were obtained from the KIND tool during Lustre programs verification<br/>
      <a href="http://www.smtcomp.org/2011/application/kind_QF_UFLIA_results_20110606.tgz">Results</a><br/>
      <a href="http://www.smtcomp.org/2011/application/kind_original_QF_UFLIA.tar">Original traces are also available, they aren't in the restricted format for competition, e.g., they use define-fun.  They also are mis-labeled as QF_UFIDL. (4.2MB)</a></li>
  <li><a href="http://www.smtcomp.org/2011/application/asasp_20110606.tgz">ASASP benchmarks (31MB)</a><br/>
 ASASP (http://st.fbk.eu/ASASP) implements a symbolic reachability
procedure for the analysis
 of administrative access control policies. A more detailed
description of the benchmarks can be found in the following paper:<br/>
 <i>Efficient Symbolic Automated Analysis of Administrative Attribute-based RBAC-Policies,</i> by F.
Alberti, A. Armando, and S. Ranise.
 <a href="http://st.fbk.eu/sites/st.fbk.eu/files/asiaccs174-alberti.pdf">http://st.fbk.eu/sites/st.fbk.eu/files/asiaccs174-alberti.pdf</a><br/>
      <a href="http://www.smtcomp.org/2011/application/asasp_20110626_results.tgz">Results (updated 26 June 2011)</a></li>
</ul>

<h2>Format for results files</h2>

<p>
Each tarball contains one or two status files for each
benchmark:
</p>

<ul>
  <li><code>BENCHMARK.results.txt</code> is always present, and</li>
  <li><code>BENCHMARK.results_untrusted.txt</code> might be present as well</li>
</ul>

<p>
These files contain one status per line, corresponding to the series of
<code>(check-sat)</code> commands in the benchmarks.
</p>

<p>
Each status line (where different from &quot;unknown&quot;) has been determined by
running at least 3 different SMT solvers on the set of instances
resulted from &quot;unfolding&quot; each incremental benchmark using the
scrambler. For <code>*.results.txt</code>, all the results different from &quot;unknown&quot;
have been reported by at least 2 different solvers, whereas all the
status lines generated by a single solver only (because e.g. the others
timed out) have been replaced with &quot;unknown&quot;. For
<code>*.results_untrusted.txt</code>, the single-solver
answer is also included. However, they are marked with an &quot;# untrusted&quot; comment.
</p>

<h2>Tools</h2>

<p>
Tools for all competition tracks are available on the
<a href="tools.shtml">tools page</a>.
</p>

<!--#include virtual="smt-comp-postlude.shtml" -->
